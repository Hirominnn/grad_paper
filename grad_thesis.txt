平成２８年度 卒業論文
深層学習を用いた 知識獲得予測を最適化する知識分類の抽出
平成２９年３月 指導教員 松尾豊 特任准教授 東京大学工学部システム創成学科知能社会システムコース
03-150984  中川大海

概要
近年，教育と情報技術の融合が進む中で，「アダプティブラーニング」という言葉が注目 されている．個人に最適化された学習内容の自動提供を実現するもので，その社会的影響 の大きさからアメリカを中心として世界的に注目が集まっており，関連するスタートアッ プや大学での研究に，多額の資金が投入されている．学習内容を個人に最適化するという 考え方自体は，決して新しいものではないが，一人の教師が複数の生徒に対して同時に教 育する形態の現在の学校教育では，全ての生徒に対して最適な学習内容を提供するには， 障壁があった．
こうした問題を，情報技術の活用によって解決しようとする動きの一つとして，アダプ ティブラーニングが浸透し始めているが，その原動力となっているのが，オンライン教育 サービスの普及である．オンライン教育サービスは，サービスを利用する生徒の学習行動 ログを収集することで，これまで困難であった大規模な学習効果分析を可能にしたことに 加え，オンライン上の学習コンテンツを生徒が個人で利用するという形態を活用し，研究 成果を元に学習コンテンツを個人に最適化して提供することを容易にした．
一方，近年，教育に限らない多くの研究領域で，深層学習が注目されている．従来の機 械学習では，人間が問題の特徴を捉えて素性を設計する必要があったが，深層学習では， 目的に応じた素性を，データから自動で学習することが可能になった．既存の機械学習の 手法を上回る性能を得られることに加え，人間が認識できないような，データの複雑な特 徴を捉えることが可能になったため，これまで人間が作り上げてきた概念を，大きく塗り 替える可能性を秘めている．こうした深層学習の技術は，オンライン学習サービスに蓄積 された大規模なデータを用いる学習効果の分析にも活用が期待されており，生徒の知識状 態を正しく把握することで，最適な学習内容を特定することを目的とする知識獲得予測の 研究に，深層学習を適用した例も報告されている．
しかし，こうした知識獲得予測の研究においては，予測アルゴリズムの部分に深層学習 を適用しているものの，素性となる「知識」は，事前に人間が作成した知識分類によって 定義されており，人間が設計した素性を利用する旧来の状況から脱していないのが現状で ある．データから特徴を自動で学習できる深層学習を活用すれば，人間が認識できないよ
2

うな，知識獲得の複雑な過程を反映した知識分類を学習できる可能性は高く，生徒の学習 効率を最適化するという最終的な目標を，真に達成するには，知識分類自体も深層学習に よって最適化される必要があるといえる．
本研究では，現在の知識獲得予測に用いられている，人間が作成した知識分類は，人間 の複雑な知識獲得過程を表現する上では最適化された表現ではない，という仮定に立ち， 知識獲得予測を行う上で最適な知識分類を，深層学習に自動的に学習させることを目的と する．実験の結果，深層学習が学習した知識分類を用いることで，人間が作成した既存の 知識分類を用いる場合よりも，高い精度で知識獲得を予測できることが検証され，深層学 習によって，知識獲得の予測性において最適化された知識分類を抽出できることが示され た．この結果は，人間が認識しきれない，人間の知識獲得の複雑な過程を説明する表現を， 深層学習が獲得したことを示しており，抽出された知識分類を活用することで，より最適 化された学習内容を生徒に提供し，生徒の学習効率を高めることができる可能性を示唆し ている．さらに，研究の拡張として，本研究で用いた分析手法の教育医学における適用の 可能性や，教育学以外の分野への応用の可能性を考察した．
本研究は．教育と情報技術の融合の進展やオンライン教育サービスの普及，教育分野に おける大規模分析の活発化や深層学習の躍進など，ここ数年の多様な領域の進展によって 初めて可能になったものである．本研究が，既存の学問体系の再構築，そして人間の学習 や知識の解明につながると信じている．
3

目次

第 1 章 序論 1.1 研究の背景 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.1.1 教育の個人最適化の重要性 . . . . . . . . . . . . . . . . . . . . . . . 1.1.2 オンライン教育サービスの普及と学習効果分析の発展 . . . . . . . . 1.1.3 深層学習の躍進 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.2 知識獲得予測の問題点 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.3 研究の目的 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1.4 本論文の構成 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .

1 1 1 2 3 4 4 5

第 2 章 関連研究

7

2.1 学習科学と情報技術 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7

2.2 オンライン教育サービスと大規模な学習効果分析 . . . . . . . . . . . . . . 8

2.2.1 MOOCs と ITS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9

2.2.2 学習行動ログの蓄積と大規模分析の活発化 . . . . . . . . . . . . . . 11

2.2.3 実証性の高いプラットフォームとしての性質 . . . . . . . . . . . . . 11

2.3 深層学習 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12

2.3.1 ニューラルネットワーク . . . . . . . . . . . . . . . . . . . . . . . . 12

2.3.2 深層学習の概要 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15

2.3.3 Recurreut Neural Networks . . . . . . . . . . . . . . . . . . . . . . 16

2.4 知識獲得の予測 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20

2.4.1 Knowledge Tracing の定式化 . . . . . . . . . . . . . . . . . . . . . 21

2.4.2 Bayesian Knowledge Tracing . . . . . . . . . . . . . . . . . . . . . 22

2.4.3 Performance Factor Analysis . . . . . . . . . . . . . . . . . . . . . 23

2.4.4 Deep Knowledge Tracing . . . . . . . . . . . . . . . . . . . . . . . 24

2.4.5 知識獲得予測の手法における DKT の最適性 . . . . . . . . . . . . . 26

2.5 次元削減手法 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28

2.5.1 Principal Component Analysis . . . . . . . . . . . . . . . . . . . . 29

i

2.5.2 Autoencoder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 2.5.3 Embedding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 2.6 用語の定義 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 2.6.1 知識獲得予測と回答正誤予測 . . . . . . . . . . . . . . . . . . . . . 31 2.6.2 知識分類と知識タグ . . . . . . . . . . . . . . . . . . . . . . . . . . 32

第 3 章 分析手法

33

3.1 分析手法全体の流れ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33

3.2 データセットの作成 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34

3.3 提案手法による知識分類の学習 . . . . . . . . . . . . . . . . . . . . . . . . 35

3.3.1 DKT の拡張による写像関数の学習 . . . . . . . . . . . . . . . . . . 35

3.3.2 写像関数の離散化による知識分類の作成 . . . . . . . . . . . . . . . 39

3.4 学習された知識分類の知識獲得予測性能に関する検証 . . . . . . . . . . . . 40

3.5 学習された知識分類の性質に関する比較分析 . . . . . . . . . . . . . . . . . 40

第 4 章 データセット

43

4.1 ASSISTments 2009-2010 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43

4.1.1 ASSISTments のサービス . . . . . . . . . . . . . . . . . . . . . . . 43

4.1.2 対象データセット . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44

4.1.3 データの抽出 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44

4.2 Bridge to Algebra 2006-2007 . . . . . . . . . . . . . . . . . . . . . . . . . . 45

4.2.1 KDDCup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46

4.2.2 対象データセット . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46

4.2.3 データの抽出 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47

4.3 データセットの概観 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48

第 5 章 実験

49

5.1 実験設定 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49

5.1.1 知識分類学習モデルによる知識分類の学習 . . . . . . . . . . . . . . 49

5.1.2 学習された知識分類の知識獲得予測の性能に関する検証 . . . . . . . 50

5.1.3 学習された知識分類の性質に関する比較分析 . . . . . . . . . . . . . 51

5.2 実験結果 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51

5.2.1 各知識分類の知識獲得予測における予測性能 . . . . . . . . . . . . . 52

ii

5.2.2 抽出タグと既存タグの関係の概観 . . . . . . . . . . . . . . . . . . . 52 5.2.3 抽出タグと既存タグの比較分析 . . . . . . . . . . . . . . . . . . . . 54

第 6 章 考察

57

6.1 知識分類学習モデルと抽出タグの解釈 . . . . . . . . . . . . . . . . . . . . . 57

6.1.1 知識分類学習モデルの有効性 . . . . . . . . . . . . . . . . . . . . . 57

6.1.2 各知識分類の性質と知識獲得予測に与える影響 . . . . . . . . . . . . 58

6.2 本手法の汎用性と実用性 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59

6.2.1 学習された知識分類の汎用性 . . . . . . . . . . . . . . . . . . . . . 59

6.2.2 本手法の他データへの適用可能性 . . . . . . . . . . . . . . . . . . . 60

6.2.3 教育現場への適用 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61

6.3 今後の展望 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62

6.3.1 知識分類学習モデルの改善 . . . . . . . . . . . . . . . . . . . . . . . 62

6.3.2 教育学における対象データの拡大 . . . . . . . . . . . . . . . . . . . 63

6.3.3 教育学以外の分野への応用 . . . . . . . . . . . . . . . . . . . . . . . 63

第 7 章 結論

65

参考文献

66

謝辞

93

iii

図目次
1.1 既存研究と本研究の差分のイメージ . . . . . . . . . . . . . . . . . . . . . . 5 2.1 Coursera のイメージ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 2.2 JMOOC のイメージ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 2.3 Knewton のイメージ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 2.4 各ニューロンの仕組み . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 2.5 単純パーセプトロンの構造 . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 2.6 多層パーセプトロンの構造 . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 2.7 RNN の基本構造 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 2.8 RNN の基本構造 (展開) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 3.1 モデル構造上の拡張 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 3.2 分析手法全体の流れ . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42 4.1 問題ごとの回答数の分布 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46 5.1 既存タグネットワークの構造 . . . . . . . . . . . . . . . . . . . . . . . . . . 53 5.2 タグ関係ネットワークの構造 . . . . . . . . . . . . . . . . . . . . . . . . . . 53 5.3 ネットワーク作成の流れ . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54 5.4 各タグの出現回数の分布 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55 5.5 多くの抽出タグが紐づく既存タグ . . . . . . . . . . . . . . . . . . . . . . . 55 5.6 少数の抽出タグのみ紐づく既存タグ . . . . . . . . . . . . . . . . . . . . . . 55 5.7 内容的関係性の強い既存タグに紐づく抽出タグ . . . . . . . . . . . . . . . . 56
iv

表目次
2.1 Deep Knowledge Tracing における回答ログデータと対応する入力ベクトル の例 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
4.1 各データセットの統計量 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48 5.1 各知識分類の知識獲得予測における予測性能 . . . . . . . . . . . . . . . . . 52 1 「ASSISTments 2009-2010」 . . . . . . . . . . . . . . . . . . . . . . . . . 75 2 「Bridge to Algebra 2006-2007」 . . . . . . . . . . . . . . . . . . . . . . . 82
v

第 1 章 序論
本章では，本論文の背景，研究目的および本論文の構成について述べる．
1.1 研究の背景
1.1.1 教育の個人最適化の重要性
近年，教育と情報技術の融合が進む中で，「アダプティブラーニング」という言葉が注目 されている．個人に最適化された学習内容の自動提供を実現するもので，その社会的影響 の大きさからアメリカを中心として世界的に注目が集まっており，関連するスタートアッ プや大学での研究に，多額の資金が投入されている [Piccioli, 2014]．
学習内容を個人に最適化するという考え方は，決して新しいものではない．現在の学校 教育では，一人の教師が複数の生徒に対して同時に教育する形態が一般的であるが，学習 の速度や教科による得手不得手は人それぞれであり，同じ教育を施しても，十分な理解が できずにつまずいてしまう生徒もいる．習熟が遅れている生徒に補習を行い，つまずいて いる原因を解明して克服する手助けをするような，生徒の学習状態を考慮して教育を設計 する試みは，常に行われてきており，そうした指導が巧みな教師は「腕のいい」教師とし て評価されてきた．しかし，こうした方法では，習熟の遅い生徒を助けることに重きが置 かれるため，習熟が周りより早い生徒への対応は後回しにされることが多く，発展的な学 習機会や知的好奇心の向上を妨げることに加え，現実的な時間と労力を考慮すると，全て の生徒に個別に対応することは困難である．
より個別に教育を受ける手段として，個別指導形式の塾や家庭教師，通信教育なども利 用される．生徒一人一人に教師がつき，生徒の習熟度合いを考慮して教育を設計できるた め，習熟の早い生徒も発展的な内容を学習することが可能で，また，能力によって優先さ れたり後回しにされたりすることもなく，学習内容を個人に最適化するという目的の上で は，より望ましい．しかし，このように，教育の粒度を細かくし，個人最適化を進めようと するほど，教師一人あたりが担当できる生徒の数が減ることによる人材的・金銭的負担や， 教師ごとの指導能力の違いなどの問題に直面する．結局，このような教師のマンパワーに
1

第 1 章 序論

2

依存した方法では，誰もが平等に最適な教育を受けるという目的を達成するには，障壁が 残る．

1.1.2 オンライン教育サービスの普及と学習効果分析の発展
こうした問題意識から，IT の活用によって，最適な学習内容を自動で提供するという アダプティブラーニングの考え方が登場したが，その原動力となっているのが，オンライ ン教育サービスの普及である．オンライン教育サービスとは，学校の教室で，一人の教師 が複数の生徒に対して同時に授業を行う，従来の教育形態と異なり，PC やモバイル端末 を通じて，オンライン上で提供される学習コンテンツを，生徒が各自で利用するサービス を指す．
オンライン教育サービスの一つである Massive Open Online Courses(MOOCs) [McAuley et al., 2010, Pappano, 2012, Siemens, 2013] は，多様な分野や難易度の講義から，時間や 場所を問わずに，生徒が自分のペースで学習したいものを選択して学習できるというもの で，従来の教育が抱える，生徒が自身の習熟度合いに沿った教育を受けられないという問 題を解決するものとして，活用が期待されている．例えば，世界最大級の MOOCs の一つ である Coursera1は，2017 年 1 月の時点で，29 の国にまたがる 148 の教育機関とパート ナーシップを結び，コンピュータサイエンス，数学や論理，社会科学などに関する 1600 以 上の講座を，2200 万人以上に提供している．日本では 2013 年 2 月に東京大学が Coursera に，2013 年 5 月に京都大学が edX2に参加を表明したことから普及し，2013 年 11 月には 日本版の MOOCs として JMOOC3が設立されるなど，国内外で MOOCs の利用が拡大し ている．
多様な講座を多くの人に提供する MOOCs 以外にも，より個人の学習過程をサポート することを目的として設計された，Intelligent Tutoring System(ITS) と呼ばれるオンラ イン自動学習支援システムの利用も拡大している．世界最大級の ITS である Knewton4で は，生徒の学力や理解度と，学ぶべき対象をマッピングすることで、生徒に最適な学習過 程を設計し，かつ生徒の学習の進捗に応じてその過程を動的に変化させる仕組みを有して いる [Upbin, 2012]．近年では ITS と MOOCs の融合も進んでおり [Aleven et al., 2015]， オンライン教育サービスの利用は世界中で拡大している．

1https://www.coursera.org/ 2https://www.edx.org/ 3https://www.jmooc.jp/ 4https://www.knewton.com/

2

第 1 章 序論

3

さらに，オンライン教育サービスは，新たな学習形態を提供するのみにとどまらず，こ れまで困難であった，大規模な学習効果分析を可能にするプラットフォームとして期待さ れている．オンライン教育サービスでは，提供された講義を生徒が学習する際に，その学 習行動ログをデータとして蓄積することが可能なため，そうして蓄積された多様な学習者 の大規模な学習行動ログから，多様な学習効果の分析が可能になった．特に，演習問題の 回答ログは，その問題が問う知識を学習者が獲得しているか否かを表すため，Knowledge Tracing と呼ばれる，知識獲得の分析に利用できる [Corbett and Anderson, 1994]．例え ば，生徒の問題回答ログを利用して知識獲得の予測を行った研究 [MacHardy and Pardos, 2015] は，有名な MOOCs の一つである Khan Academy5に蓄積された 100 万件以上の問 題回答ログを使用しており，教育の分野における大規模な学習効果分析の一つである．生 徒が個人で利用するというオンライン教育サービスならではの特性によって，このような 研究成果を元に最適化された学習コンテンツを，個人に提供することが容易になったため， 教育の個人最適化を進める動きは，急速に活発化している．

1.1.3 深層学習の躍進
一方，近年，教育に限らない多くの研究領域で，深層学習が注目されている．従来の機 械学習では，人間が問題の特徴を捉えて素性を設計する必要があったが，深層学習では，目 的に応じた素性を，データから自動で学習することが可能になり，画像認識 [Schroﬀ et al., 2015, Szegedy et al., 2014]，音声認識 [Hinton et al., 2012, Bahdanau et al., 2015]，機械 翻訳 [Sutskever et al., 2014, Dong et al., 2015] 等，多様な研究領域で飛躍的な進展が報告 がされている．直近の一年間では，画像から動画を生成する研究 [Vondrick et al., 2016] や，会話を人間と同程度に認識できるとする音声認識の研究 [Xiong et al., 2016a]，一部 の欧米言語間の文レベルで，ほぼ人間と同等に正確な翻訳を実現したとする機械翻訳の研 究 [Wu et al., 2016] なども報告されており，深層学習によって，日々驚異的な成果が生み 出されている．また，2016 年 3 月に人間のプロを倒したことで一躍有名になった，Google Deep Mind が開発したコンピュータ囲碁プログラムの「AlphaGo」[Silver et al., 2016] は， 過去の人間の対局の記録である棋譜から，深層学習によって人間の以後の打ち方を学習し， コンピュータ同士の対局による強化学習を通して，今後１０年は不可能と言われていた， 人間のプロを打ち負かすほどの棋力を獲得した．AlphaGo は，単に人間の打ち手を真似 ただけでなく，それまで人間が考えつかなかったような手を学習しており，囲碁界に衝撃

5https://www.khanacademy.org/

3

第 1 章 序論

4

を与えている．このように，深層学習は，人間が認識できないようなデータの複雑な特徴 を捉えることで，これまで人間が作り上げてきた概念を，大きく塗り替える可能性を秘め ている．
こうした深層学習の技術は，オンライン教育サービスに蓄積されたデータを元にした， 学習効果の分析にも活用が期待されている．特に，オンライン教育サービスの普及によっ て盛んになった Knowledge Tracing の研究も，深層学習の適用によって大きく進展した． Piech らが発表した，Knowledge Tracing に深層学習を活用する Deep Knowledge Tracing という手法では，時系列分析でよく用いられる深層学習モデルである Recurrent Neural Networks [Williams and Zipser, 1989] を活用することで，高い性能で知識獲得を予測で きること，また，予測モデルを分析することで知識間の関係性をネットワークとして抽出 できることが報告されている [Piech et al., 2015]．

1.2 知識獲得予測の問題点
しかし，こうした知識獲得予測に深層学習を適用する研究においては，ある問題が存在 する．知識獲得を予測するアルゴリズムの部分には深層学習を適用しているものの，素性 となる「知識」は，事前に人間が作成した知識分類によって定義されており，人間が設計 した素性を利用する旧来の状況から脱していない．人間が作成した分類というのは，「知識 の体系はこうなっているはずだ」ないしは「この体系に基づいて教える・教えられること が望ましい」という専門家の仮説や理論に基づいて作られたものである．そのため，人間 にとっての可読性は高いとしても，実際の生徒の知識獲得の過程を定量的に分析した上で， 知識獲得の予測性が最適化されているという根拠はない．今日多様な分野で成果を生んで いる，データから特徴を自動で学習できる深層学習を活用すれば，人間が認識できないよ うな，人間の知識獲得の過程に潜む複雑な構造を反映した，予測を最適化するような知識 分類を学習できる可能性は高く，生徒の学習効率を最適化するという最終的な目標を真に 達成するには，知識分類自体も深層学習によって最適化される必要があるといえる．
以上の問題意識に基づいた，既存研究との本研究の差分のイメージを図 1.1 に示す．
以上，本研究の背景について述べた．次に，上記の背景を踏まえた研究目的について説 明する．

4

第 1 章 序論

5

図 1.1: 既存研究と本研究の差分のイメージ
1.3 研究の目的
本研究では，現在の知識獲得予測に用いられている，専門家が作成した知識分類は，人 間の知識獲得の予測を行う上では最適化された表現ではない，という仮定に立ち，下記を 検証することを目的とする．
• 深層学習によって抽出した知識分類を用いることで，専門家が作成した知識分類を 用いる場合よりも，高い精度で知識獲得予測を行うことができる．
本論文では，専門家による事前の知識分類を所与のものとせずに，問題の回答ログデー タのみを深層学習に適用し，回答正誤予測を行う過程で，その予測性を最適化する知識分 類を抽出することを目的としている．従来，専門家が事前に分類することが必要であった 生のデータから，予測性を最適化する知識分類を，深層学習によって自動で抽出できるこ とを明らかにすることは，既存の学問体系やカリキュラム設計の再構築につながるだけで なく，人間の知識状態の推移するメカニズムを解明することにもつながり，学術的な意義 が大きいと考える．
以上，本研究の目的について述べた．次に，背景と目的を踏まえて本論文の構成につい て述べる．
5

第 1 章 序論

6

1.4 本論文の構成
以降の本論文の構成について述べる． 2 章では，関連研究について述べる．知識獲得予測の関連研究を俯瞰し，周辺概念を整 理することで，本研究の学術的位置づけを明確にする． 3 章では，分析手法について述べる．まず，生徒の回答正誤を予測する過程で最適な知 識分類を学習する手法について説明し，また，その結果から抽出された知識分類を分析す る手法について説明する． 4 章では，実験で用いるデータセットについて述べる．３章で述べたデータセットとし ての要件を満たす，オンライン教育サービスにおける生徒の学習回答ログである，２つの データセットを紹介し，本研究に適用するための事前の処理を説明する． 5 章では，実験について述べる．本研究で行う実験を大きく３つに分け，各実験の設定 を述べた後，実験結果について述べる．実験結果においては，提案手法によって抽出され た知識分類を用いることで，既存の知識分類を用いた場合よりも高い精度で知識獲得を予 測できることを示し，予測性を最適化する知識分類が抽出されたことを定量的に確認する． さらに，抽出された知識分類を既存の知識分類と定量的・定性的に比較することで，その 性質を解釈する． 6 章では，実験結果を踏まえた考察を述べる．まず，本研究で用いた手法や抽出された 知識分類について，実験結果を踏まえて，その性質を考察し，実際の教育現場への適用法 について述べる．また，本研究で用いた分析手法の，多様なデータへの適用可能性につい て議論し，教科によらず適用できる可能性があること，教科によって抽出される知識分類 の性質が異なる可能性があること，一方で，複合的な学問や専門性の高い学問については， 適用可能性の検証実験が必要であることを述べる．さらに，本研究で抽出された知識分類 を離散表現に改めることの重要性や可能性について論じ，また，本研究で用いた分析手法 が，教育学以外の分野にも応用できる可能性を持つことを論じる． 最後に，7 章で結論を述べる．
以上，序論について述べた．次に，関連研究について述べる．

6

第 2 章 関連研究
本研究が問題提起を行った知識獲得予測の研究は，学習科学の発達や，教育に関する大 規模分析の活発化，深層学習技術やその他の多様な分析技術の進展により発展してきた研 究分野である．本章では，知識獲得予測の関連研究を俯瞰し，現状の環境や周辺概念を整 理することで，本研究の学術的位置づけを明確にする．
まず，人間の学習効果について研究する学習科学の歴史と情報技術の発達との関係性に ついて整理した後，その情報技術の一つとして，教育の個人最適化を解決すると期待され ているオンライン教育サービスについて，具体的な事例を挙げながら，その効果や関連する 研究について述べる．次に，深層学習について概説し，本論文との関わりが深い Recurrent Neural Netoworks について詳細に述べる．さらに，知識獲得の予測手法である Knowledge Tracing について，その有益性や，伝統的な手法，深層学習を用いた最先端の手法につい て整理した後，本研究において既存手法を拡張する上で用いる，大規模データから次元削 減を行う関連手法について整理する．最後に，以上の関連研究を踏まえて，本論文で使用 する類似の用語について，定義を明確にする．
2.1 学習科学と情報技術
20 世紀後半，人間の心の働きを理論化する認知科学が，現実社会で実際に役立つ科学 として再構築される流れの中で，人を日常の学びの中で今より賢くするために実際に役立 つ科学として，「学習科学 (Learning Sciences)」の分野が確立された [白水始 et al., 2014]． 学習科学は，従来の，実験室環境でのみ確認されるような非実用的な理論研究を避け，学 習がうまくいく要因や状況を解明した上で，その学習を人間が積極的に引き起こすことを 目指すような，実践の学を新たに打ち立てることを目指したものであった．明確な定義は 様々であるが，[三宅なほみ et al., 2002] らは 2002 年に「よりよい教育を実現したいとい う社会的要請を背景にして，これまでの認知研究に基づき，現実の人の学習，例えば学校 教育の中での子どもたちの学習を研究し，現代のテクノロジを駆使して実効性のある教育 のシステムを教育実践の中で作り上げようという研究動向」と定義している．
7

第 2 章 関連研究

8

学習科学の発展は，認知科学の進展だけでなく，情報技術の発達が大きな貢献を果たし ている．学習科学は，人間の認知過程を解明する基礎研究としての性質に加え，実社会で の有効性を検証する実証的な応用研究としての性質を兼ね備えているため，オンライン教 育サービスのような教育と情報技術の融合によって，これまで実現しにくかった学習環境 を作り検証できるようになったことは，学習科学の発展を大きく加速させた．
そうした情報技術との融合により実証研究が進み，今日注目されている分野の例として 「アダプティブラーニング」が挙げられる [Carbonell, 1970, Midgley, 2014]．アダプティ ブラーニングは，個人に最適化された学習内容の自動提供を実現するもので，その社会的 影響の大きさからアメリカを中心に注目が集まっており，関連するスタートアップや大学 での研究に，多額の資金が投入されている [Piccioli, 2014]．
学習内容を個人に最適化させるという考え自体は，学習科学研究の分野でも，また，研 究という形に上がらないレベルでも，古くから存在し，例えば習熟の遅い生徒に教師が個 別で補習に当たったり，個別指導塾や通信教育で生徒各自が自身の習熟度にあった講義を 受けたりと，様々な形態を取って実践されてきた．しかし，こうした従来の方法は，教育 の粒度を細かくし，個人最適化を図ろうとするほど，教師一人あたりが担当できる生徒の 数が減ることによる人材的・金銭的負担や，教師ごとの指導能力の違いなどの問題に直面 し，誰もが平等に最適な教育を受けるという目的を達成するには，障壁が残っていた．
この事態を打開したのが，教育と情報技術の融合である．中でも，その象徴とも言える オンライン教育サービスでは，サービスを利用する生徒の学習行動ログを収集することで， これまで困難であった大規模な学習効果分析を可能にしたことに加え，オンライン上の学 習コンテンツを生徒が個人で利用するという形態を活用し，研究成果を元に学習コンテン ツを個人に最適化して提供することを容易にした．
このように，基礎理論に加え実証性も重視する学習科学の領域は，情報技術との融合に より大きく発達してきた．特に，オンライン教育サービスは，データの蓄積と研究成果の 実証という二つの目的が達成できるプラットフォームとして，大きな注目を集めている．
以上，学習科学と情報技術の関係性について述べた．次に，教育と情報技術の融合を象 徴する例として，オンライン教育サービスについて述べる．

2.2 オンライン教育サービスと大規模な学習効果分析
オンライン教育サービスの代表的な例として Massive Open Online Courses(MOOCs) と Intelligent Tutoring System(ITS) を取り上げ，具体的な事例を挙げながら，関連する
8

第 2 章 関連研究

9

研究について述べる．また，こうしたオンライン教育サービスが大規模分析に活用されて いる状況や研究について整理する．

2.2.1 MOOCs と ITS
MOOCs は Massive Open Online Courses [McAuley et al., 2010, Pappano, 2012, Siemens, 2013] の略称で，特に日本語で表記する場合は大規模公開オンライン講座と記 述することがある．MOOCs は，オンライン上で公開された，大学を始めとする様々な教 育機関などの講座を，誰もが無償で受講でき，また修了時には修了証も取得できる教育 サービスのことを指す．
学びたい人が，いつでもどこでも学習リソースにアクセスできるという MOOCs の概念 自体は古くから提唱されていたが，実現化したのは，2008 年にカナダのマニトバ大学で学 生向けのオンライン講座を開設した際に，25 人の受講者だけでなく 2000 人以上の人がそ の講座に参加したことがきっかけだと言われている [Yuan et al., 2013]．
以前から，大学などの高等教育機関は，オープンコースウェア [Abelson, 2008] という 形で講義の動画や資料を公開していたが，MOOCs は，参加人数が非常に大規模で，また， 高等教育水準の内容だけでなく，初等中等教育水準の内容の講座も含まれている点で異な る．また，これまでもオンラインの講座というものは存在していたが，MOOCs は，参加 人数が非常に大規模である点や公開している講座の数が大規模である点，また，その内容 が多様であるという点，利用が無料，あるいは無料に近いという点において，これまでの オンライン講座とは異なる．
MOOCs は，従来の，学校の教室で一斉授業形式で提供される教育形態と異なり，オン ライン上の多様な講座に，生徒が個人でアクセスし，講座ごとに提供される講義の動画や 演習システムなどを通じて，いつでもどこでも，自身の習熟度合いやペースに合わせて， 自分の学習したいものを選択して学習できる．従来の教育の，生徒が自身の習熟度合いに 見合った学習ができないという問題を解決するものとして注目されていることに加え，産 業や社会への影響も注目されている．例えば，大学生だけでなく社会人も，自身の専門領 域に関する講座を受講することでより理解を深めたり，あるいは専門領域とは異なる幅広 い講座を受講することで，教養を養うことができる．また，公教育の整備が追いついてい ないような発展途上国においては，MOOCs が教育に与える影響は大きく，その影響や可 能性を分析する報告は多い [Trucano et al., 2013, Liyanagunawardena et al., 2013]．
このように，MOOCs は社会の多様な場面で，これまでにない学習機会を提供しており，

9

第 2 章 関連研究 教育や学習といったもののあり方に大きな影響を与えている．

10

図 2.1: Coursera のイメージ

図 2.2: JMOOC のイメージ

MOOCs の有名な事例として，世界的に有名な Coursera や，日本発の MOOCs である JMOOC が挙げられる．Coursera と JMOOC のイメージを図 2.1，2.2 に示す．Coursera は，2017 年 1 月の時点で，29 の国にまたがる 148 の教育機関とパートナーシップを結び， コンピュータサイエンス，数学や論理，社会科学などに関する 1600 以上の講座を，2200 万人以上に提供している1．JMOOC は，2013 年 11 月に日本版の MOOCs として設立さ れ，10 代から 80 代までと幅広い年代に，アートや医療，自然科学や資格試験対策などの 講座を提供しており，2017 年 1 月の時点で，140 の講座を 50 万人以上が受講している2．
多様な講座を多くの人に提供する MOOCs 以外にも，より個人の学習過程をサポートす ることを目的として設計された，Intelligent Tutoring System(ITS) と呼ばれるオンライン 自動学習支援システムの利用も拡大している [Sleeman and Brown, 1982]．
ITS の有名な事例として，世界最大級の ITS である Knewton3のイメージを図 2.3 に示 す．Knewton では，生徒の学力や理解度と，学ぶべき対象をマッピングすることで、生徒 に最適な学習過程を設計し，かつ生徒の学習の進捗に応じてその過程を動的に変化させる 仕組みを有している [Upbin, 2012]．
また，近年では，これまで難しいと言われていた ITS の MOOCs への埋め込みを達成 したとする研究 [Aleven et al., 2015] も報告されており，ITS が利用される場面は，今後 より拡大していくといえる．
1講座数と利用者数はトップページの記載より引用． 2講座数と利用者数はトップページの記載より引用． 3https://www.knewton.com/
10

第 2 章 関連研究

11

図 2.3: Knewton のイメージ
2.2.2 学習行動ログの蓄積と大規模分析の活発化
MOOCs や ITS を始めとするオンライン教育サービスは，人々に新たな学習の機会を提 供するという側面だけでなく，これまで難しかった大規模な学習効果分析の可能性を高め るという側面もある．
生徒はオンライン上で提供された講義動画や演習問題を通して学習するが，オンライン 上で実施されているため，学習行動ログをデータとして蓄積することができ，蓄積された データを分析に活用することができる．多様な生徒が利用するため，多様な生徒の大規模 な学習行動ログから多様な講座の学習効果の分析が可能となりつつある．
特に，演習問題の回答ログはその演習問題により評価される知識を生徒が獲得している か否かを表現しているため，知識獲得の分析に利用できる．例えば，MOOCs の演習問題 の回答ログを利用して知識獲得の予測を行う研究 [MacHardy and Pardos, 2015] では，世 界的に有名な MOOCs である Khan Academy から収集したデータを利用していたが，そ の問題回答ログ数は 100 万件以上であり，これまでにないほど大規模なデータを対象に分 析が実施されたといえる．
2.2.3 実証性の高いプラットフォームとしての性質
さらに，オンライン教育サービスが学習効果分析の価値を大きく高めている要因として， オンライン上のコンテンツを，多様な生徒が，個人で利用するというプラットフォームと しての性質がある．
11

第 2 章 関連研究

12

現在の学校教育の形態では，生徒の学習効果に関する分析を行い，なんらかの知見を得 たとしても，それを多様な生徒に適用して効果を検証したり，各個人に提供できるような 環境が整備されておらず，学習効果分析が社会に与える影響が限定的であった．また，従 来の一般的な e ラーニングによる学習支援システムも，大学のような各教育機関が個別に 設定し，学内の生徒が利用者の中心であったため，システムの利用者が限定されており， データの多様性や研究成果の活用可能性も狭い範囲に留まっていた．
一方，MOOCs や ITS のような大規模なオンライン教育サービスは，教育機関の垣根に とらわれず，多様な背景，適性，能力を持つ生徒が利用していることに加え，学習コンテ ンツを個人が利用する形態のため，多様なデータを元に得られた一般性のある知見を，多 様な生徒に対して，生徒個人の粒度で提供することが可能である．例えば，生徒の知識獲 得予測の研究は，得られた成果から，より生徒の学習効率を高めたり，継続を推進するよ うな教材推薦システムを開発し，実際のサービス上で個人個人に適用することで，効果を 実証することができる．このような性質から，オンライン教育サービスのデータに基づい た学習効果分析が持つ社会的影響は，大きなものとなっている．
以上，オンライン教育サービスを取り巻く環境と，学習効果分析との関連性について述 べた．
次に，深層学習について述べる．

2.3 深層学習
本研究で用いる技術の核となっている，深層学習について述べる．まず，深層学習の基 礎となっているニューラルネットワークの概念について説明した後，深層学習の概要につ いて述べ，さらに本研究で用いる深層学習モデルである Recurrent Neural Networks につ いて詳述する．
2.3.1 ニューラルネットワーク
深層学習は，機械学習における一分野であり，その中でもニューラルネットワークとい う特殊なモデル構造を拡張したものである．よって，まずはニューラルネットワークにつ いて説明する．
ニューラルネットワークは，機械学習におけるモデル構造の一つで，人間の脳の神経回 路の仕組みを模したものである．人間の脳は，膨大な数のニューロンと呼ばれる神経細胞
12

第 2 章 関連研究

13

から構成され，各ニューロンは相互に連結し，巨大なネットワークを成している．外界か らの情報によってあるニューロンが刺激を受けると，そのニューロンの電位は次第に上昇 し，電位が一定の閾値を超えるとそのニューロンは発火し，接続している他のニューロン に情報の信号を出力することにより，情報の伝達が行われている．ニューラルネットワー クのモデルでは，このニューロン一つ一つの情報伝達の仕組みと，それらが互いに接続し てネットワークを成す構造をモデル化している．
各ニューロンは図 2.4 のようにモデル化される．各ニューロンは他のニューロンから入力 信号 x を受け取るが，その信号の伝達効率は一様ではない．それぞれの入力には伝達効率 として重み w が設定され，その重み付きの入力 wx が対象のニューロンに加算されていく． その総和 u は事前に定められた活性化関数 f に基づいて正規化され，出力される．活性化 関数には様々な種類があり，式 2.1 で表されるような，閾値 θ を境に０か１を出力するよ うな単純なステップ関数以外に，式 2.2, 2.3 で表されるようなシグモイド関数 (sigmoid)， 双曲線正接関数 (tanh) などの非線形関数も存在する．

  1if x ≥ θ φ(x) = f (x − θ) =  0if x < θ

(2.1)

1 φ(x) = 1 + e−x

(2.2)

ex − e−x φ(x) = ex + e−x

(2.3)

図 2.4: 各ニューロンの仕組み このようにモデル化された各ニューロンを，人間の脳の神経回路のように，互いに結合
13

第 2 章 関連研究

14

図 2.5: 単純パーセプトロンの構造

図 2.6: 多層パーセプトロンの構造

させてネットワーク化した例が図 2.5 である．これは 1958 年に Rosenblatt により提案さ れた単純パーセプトロンというネットワークで，二ューラルネットワークの元祖とも言わ れる最も基本的なネットワークである [Rosenblatt, 1958]．それぞれのニューロン（以下， ユニット）は，各層の間で互いに全結合しており，前の層からの入力 x に重み W が掛け 合わされ，バイアス項 b を加算したものに活性化関数 f が適用され，出力される．層 i か ら層 j への出力の計算は以下の式に基づいて行われる．

yj = f (Wjxi + bj

(2.4)

なお．xi は層 i における出力 (i = 1 の時はモデルへの入力) を指し，Wj は重み行列を指 し，bj はバイアス項を指し，f は活性化関数を指し，yj は層 j における出力を指す．
この計算を層ごとに順次行い，最終的な出力が決定され，事前に設定した誤差関数に よってモデルの予測誤差が算出される．ニューラルネットワークでは，この誤差関数を重 みやバイアスのパラメータによって微分して負の勾配方向を見つけ，パラメータを勾配方 向に修正することを繰り返すことにより，最適なパラメータが得られるように学習を進め る．重みの更新の基本的な仕組みは，以下の式によって表される．

∇E

=

∂E

∂E =[ ,

∂E , · · · ,

∂E

]

∂w ∂w1 ∂w2

∂wn

w(t + 1) = w(t) − ϵ∇E

(2.5) (2.6)

なお，E は誤差関数であり，w(t) は時刻 t における重みベクトルであり，ϵ は学習率であ り，パラメータ更新の程度の大きさを決定する定数である．
ニューラルネットワークの基礎として考案された単純パーセプトロンだが，排他的論理

14

第 2 章 関連研究

15

和 (XOR) のような非線形問題を解けず，現実の複雑な問題には適用できないことが指摘さ れた [?]．また，ニューラルネットワークを多層にすることも早くから考案されていたが， 極めて高い計算処理性能を要することが課題であり，長い間実用には堪えない時代が続い ていた．このような歴史を経た後，近年の計算機の性能向上や，その他のモデル設計上の 技術的進歩を背景に，ニューラルネットワークをさらに多層に重ねて，より複雑な特徴を 抽出し，非線形問題も含めた様々な問題を扱えるように設計されたのが深層学習である．

2.3.2 深層学習の概要
深層学習は，機械学習における一分野で，ニューラルネットワークと呼ばれるモデル構造 を多層に重ねたものである．画像処理に利用される Convolutional Neural Networks [?] や 系列データの処理に利用される Recurreut Neural Networks [Williams and Zipser, 1989] など，目的に応じた様々な拡張があるが，どれも図 2.6 のような多層パーセプトンという モデル構造を基本としている．多層パーセプトロンも，層間の信号の伝搬など，基本的な 構造は単純パーセプトロンと大きな違いはない．しかし，隠れ層が多層になったことで， 何層にも渡って微分の連鎖規則を繰り返すことが必要になり，計算コストが膨大で現実的 でない．そのため考案されたのが誤差逆伝搬法 [Rumelhart et al., 1988] である．誤差逆 伝搬法では，出力結果に基づいて，出力層から入力層に向かって順番に重みを修正する手 法により，複雑な問題を説明するような，ユニット間の重みを学習できるようになり，非 線形問題も解くことが可能になった．
この誤差逆伝搬法や，ミニバッチ勾配降下法や確率的勾配降下法などの効率的な勾配降 下法，過学習を抑制するドロップアウトなど，様々な技術的工夫が考案されたことにより， 現実的な計算コストで効率的に深層学習を行うことが可能になり，深層学習を用いた研究 活動が急速に活発化した．
深層学習の活用により，画像認識 [Schroﬀ et al., 2015, Szegedy et al., 2014]，音声認 識 [Hinton et al., 2012, Bahdanau et al., 2015]，会話認識 [Sak et al., 2015]，機械翻 訳 [Sutskever et al., 2014, Dong et al., 2015]，質問応答文生成 [Yin et al., 2015]，画像説 明文生成 [Xu et al., 2015, Vinyals et al., 2014] 等，多様な研究領域で飛躍的な進展が報告 がされている．特に，直近の一年間だけでも画像から動画を生成する研究 [Vondrick et al., 2016] や，会話を人間と同程度に認識できるとする音声認識の研究 [Xiong et al., 2016a]， 一部の欧米言語間の文レベルで，ほぼ人間と同等に正確な翻訳を実現したとする機械翻訳 の研究 [Wu et al., 2016] などを始めとする数々の報告がされており，深層学習によって，

15

第 2 章 関連研究

16

日々驚異的な成果が生み出されている． また，2016 年 3 月に人間のプロを倒したことで一躍有名になった，Google Deep Mind
が開発したコンピュータ囲碁プログラムの「AlphaGo」[Silver et al., 2016] は，過去の人 間が打った大量の棋譜に深層学習を適用した後，コンピュータ同士の対局による強化学習 を通して，今後１０年は不可能と言われていた，人間のプロを打ち負かすほどの棋力を獲 得した．AlphaGo は，過去の対局の情報である棋譜の分析によって人間を真似ただけでな く，それまで人間が考えつかなかったような手を学習しており，囲碁界に衝撃を与えてい る．このように，深層学習は，人間が認識できないようなデータの複雑な特徴を捉えるこ とで，これまで人間が作り上げてきた概念を，大きく塗り替える可能性を秘めている．
一般に，深層学習モデルを学習させる際には，大規模な訓練データが必要となる．深層 学習モデルが，人の手で素性を設計していない生の訓練データから，特徴的な表現を学習 し，最適化するには，膨大な数の内部パラメータを設定して学習することが必要で，とき には数十万から数百万以上の内部パラメータが設定されることもあり，こうした膨大な数 のパラメータを学習するには，大規模な訓練データが必要となる．データ数が不足すると， データの潜在的な特徴を十分に学習できないことに加え，汎用性の低い特徴まで過剰に学 習してしまう「過学習」に陥りやすくなる [Tetko et al., 1995]．
実際に大規模データを利用した研究の例を挙げると，人間より高い精度で人の顔を見分 けらると報告する顔認識の研究 [Schroﬀ et al., 2015] では，数百万人の 2 億枚以上の顔画像 を訓練データに利用している．英語からフランス語に翻訳する機械翻訳の研究 [Xu et al., 2015] では，1200 万もの文章を訓練データとして利用している．

2.3.3 Recurreut Neural Networks
深層学習のネットワークには，目的に応じたいくつかの種類があるが，ここでは，知識 獲得の予測に深層学習を用いた手法 [Piech et al., 2015] に用いられていたニューラルネッ トワークである，Recurreut Neural Networks [Williams and Zipser, 1989]（以下，RNN） について説明する．
RNN は深層ニューラルネットワークの一種で，主に系列データの解析に利用される．系 列データとは，同質のデータを直列に並べて表現することにより，特定の意味を持ったデー タのことで，例えば，時系列に沿って変化する株価のようなデータや，一定の長さで順序 を持って並ぶ単語から構成される文章などのデータが系列データにあたる．
近年，RNN はデータの大規模化や計算機性能の向上などにより，幅広い領域の系列

16

第 2 章 関連研究

17

図 2.7: RNN の基本構造

図 2.8: RNN の基本構造 (展開)

データに対して適用されるようになった．具体的には，機械翻訳 [Sutskever et al., 2014, Dong et al., 2015]，手書き文字認識 [Graves and Schmidhuber, 2009, Louradour and Kermorvant, 2014]，音声認識 [Hinton et al., 2012, Bahdanau et al., 2015]，ユーザログ 解析 [Hidasi et al., 2015]，画像説明文生成 [Xu et al., 2015, Vinyals et al., 2014]，医療診 断 [Choi et al., 2015, Lipton et al., 2015] 等の領域で高い性能を発揮することが報告され ている．
伝統的な RNN は，入力層，隠れ層，出力層の 3 層から構成されている．系列方向を時刻 とすれば，時刻 t の隠れ層 ht の計算に時刻 t−1 の隠れ層の情報を入力する ht = f (xt, ht−1) という式のように，一つ前の情報を繰り返し（recurrent）入力するという構造である．モ デル構造は図 2.7 のように表され，隠れ層の部分を時間方向に展開して図 2.8 のように表さ れることもある．関数 f は，入力である xt や ht−1 をアフィン変換4して足しあわせた後， 活性化関数にかけるというものがよく利用される．活性化関数はシグモイド関数や tanh （Hyperbolic Tangent 関数），Relu [Nair and Hinton, 2010]，ELUs [Clevert et al., 2015] など多く提案されており，通常，非線形関数である．
このように，データの系列に沿った情報を反映して学習できる RNN だが，課題の一 つとして，長期的な表現になるほど学習が難しくなるということが挙げられる [Bengio et al., 1994]．RNN の学習には，勾配法に基づいた確率的勾配降下法 [Robbins and Monro, 1951, Kushner and Yin, 2003] や Adam [Kingma and Ba, 2014]，AdaDelta [Zeiler, 2012] など，さまざまな手法が利用可能である．しかし，いずれの勾配法を用いるにせよ，勾配 が爆発して学習モデルが壊れてしまうという勾配爆発 [Bengio et al., 1994, Pascanu et al., 2013] という問題や，勾配が消滅して対象データの長期的な特徴量を捉えることができな いという勾配消滅 [Pascanu et al., 2013, Hochreiter, 1998] という問題がしばしば発生す る．これは，ht = f (xt, ht−1) の式に表れるように同じ変換を繰り返し行うためであり，こ
4平行移動と線形変換を組み合わせた変換のこと．
17

第 2 章 関連研究

18

のため，特に長い系列データを RNN で学習する場合，効果的に長期的な表現を学習させ ることが難しい．
こうした問題を解決もしくは緩和するため，学習時の勾配に制約を加える方法やゲート 付き活性化関数の利用が提案されている．まず，勾配爆発の緩和に対しては，学習時の勾 配に制約を加える方法が有効である．具体的には，[Mikolov, 2012] では学習させるパラ メータの勾配の絶対値の最大値を予め決めておき，最大値以上の場合には，勾配の最大値 になるように勾配の値を置き換えることで勾配爆発の影響を緩和する方法が報告された． また，[Pascanu et al., 2013] では学習させるパラメータの勾配のノルム5の最大値を予め 決めておき，最大値以上の場合には， ノルムが最大値以下になるように疑似コード 1 に 従いノルムを抑制することで勾配爆発の影響を緩和する方法が報告された．

Algorithm 1 勾配爆発を防ぐための勾配ノルム抑制の疑似コード

gˆ

←

δε δθ

if ∥gˆ∥ ≥ threshold then

gˆ

←

threshold ∥gˆ∥

gˆ

end if

次に，勾配消滅の緩和に対しては，ゲート付き活性化関数の利用が有効である．先に言 及したが，RNN には異なる活性化関数を利用するという形でいくつかの種類がある．うま く設計された活性化関数を利用することで，勾配消滅を緩和してデータの長期的な特徴を よく捉えられたり，計算コストを削減することができたりする．以降では，よく研究報告 で取り上げられる Simple RNN（以下，SRNN）[Williams and Zipser, 1989]，Long Short Term Momory RNN（以下，LSTM-RNN）[Hochreiter and Schmidhuber, 1997]，Gated Recurrent Neural Networks（以下，GRNN）[Cho et al., 2014] の 3 つについて詳細に説 明する．

SRNN SRNN はゲート付き活性化関数を用いない簡単な構造の RNN である．[Le et al., 2015,
Krueger and Memisevic, 2015] で報告される工夫を取り入れることで，データの長期的な 特徴を効果的に捉えることができるようになるが，多くの場合で，LSTM-RNN や GRNN のようにゲート付き活性化関数を用いる RNN の方がモデルの性能という点で優れている．
SRNN によるモデルの定式はいくつか種類が存在するが，シンプルなものは例えば下記
5ベクトルの「長さ」の概念を一般化したもの
18

第 2 章 関連研究

19

の式で定義される．

ht = tanh(Wxhxt + Whhht−1 + bh) yt = σ(Whyht + by)

(2.7) (2.8)

ここでは，t は時刻を指し，xt は時刻 t の入力ベクトルを指し，ht は時刻 t の隠れ層を指 し，yt は時刻 t の入力ベクトルを元にした予測値を指し，Wxh，Whh はそれぞれ重み行 列を指し，bh，by はそれぞれバイアス項を指し，tanh は (ex − e−x)/(ex + e−x) で定義さ れる Hyperbolic Tangent 関数を指し，σ は 1/(1 + e−x) で定義されるシグモイド関数を指 す．訓練時には，重み行列 Wxh，Whh とバイアス項 bh，by を学習する．

LSTM-RNN
LSTM-RNN は Long Short Term Memory という活性化関数を用いる RNN で，その名 前の通り，SRNN では捉えることが難しかったデータの長期的表現と短期的表現の両方の 獲得を目的に開発されたものである [Hochreiter and Schmidhuber, 1997]．LSTM-RNN は SRNN と比較すると，モデルの性能という点で優れているが，内部のパラメータの数 が非常に大きく学習コストは大きい．最先端の成果を報告する研究でしばしば利用されて いるが，LSTM-RNN 自体が開発されたのは 1997 年であり LSTN-RNN が新しいというわ けではない．
LSTM-RNN によるモデルの定式にはいくつか種類が存在するが，特に，後述する Deep Knowledge Tracing [Piech et al., 2015] で用いられる LSTM-RNN は下記の式で定義さ れる．

it = σ(Wxixt + Whiht−1 + bi) gt = σ(Wxgxt + Whght−1 + bg) ft = σ(Wxf xt + Whf ht−1 + bf ) ot = σ(Wxoxt + Whoht−1 + bo) mt = ft ⊙ mt−1 + it ⊙ gt ht = ot ⊙ mt yt = σ(Wmymt + by)

(2.9) (2.10) (2.11) (2.12) (2.13) (2.14) (2.15)

ここでは，it は Input Gate を指し，ft は Forget Gate を指し，gt はメモリセルへの入力 19

第 2 章 関連研究

20

を指し，ot は Output Gate を指し，mt はメモリセルを指し，Wxi，Whi，Wxg，Whg， Wxf ，Whf ，Wxo，Who，Wmy はそれぞれ重み行列を指し，bi，bg，bf ，bo，by はそ れぞれバイアス項を指し，⊙ は要素積を指す．
式 2.13 にあるように，メモリセルへの入力は 1 つ前のメモリセルの状態 mt−1 と入力 gt であり，それぞれの入力に対して，過去のメモリセルからの情報を捨てる Forget Gate と現在からの情報を調整する Input Gate を作用させ，mt を得る．新しい隠れ層 ht は式 2.14 のようにメモリセルからの出力を Output Gate で調整したものを入力として受け取 る．これらのゲートにより，長期的な特徴と短期的な特徴が捉えられるとされている．

GRNN
GRNN は Gated Recurrent Unit [Cho et al., 2014] というゲート付き活性化関数を用い る RNN のことで，GRU は LSTM のように，長期的な表現と短期的な表現を捉えるために 提案された活性化関数である．Cho ら [Cho et al., 2014] が 2014 年に発表して以来，GRNN 自体や GRNN の活用に関する研究が多く報告されている [Chung et al., 2014, Zaremba, 2015, Chung et al., 2015, Karpathy et al., 2015, Biswas et al., 2015, Pezeshki, 2015]． LSTM よりもゲートの数が少なく学習コストが小さい傾向にあるが，LSTM-RNN，GRNN の性能を比較した研究 [Chung et al., 2014, Zaremba, 2015] において LSTM-RNN と GRNN が同程度の性能であることが報告されている．
GRNN は下記の式により定義される．

rt = σ(Wxrxt + Whrht−1 + br) zt = σ(Wxzxt + Whzht−1 + bz) h˜t = tanh(Wxhxt + Whh(rt ⊙ ht−1 + bh)) ht = zt ⊙ ht−1 + (1 − zt) ⊙ h˜t yt = σ(Whyht + by)

(2.16) (2.17) (2.18) (2.19) (2.20)

ここでは，Wxr, Whr, Wxz, Whz, Wxh, Whh は重み行列で，br, bz, bh はバイアス項で ある．rt が Reset Gate(LSTM における Forget Gate に相当する機構) で，zt が Update Gate(LSTM におけるメモリセルに相当する機構) である．rt が 0 に近いほど前の隠れ層 からの入力よりも現在の入力をより強く考慮するようになり，zt が 0 に近いほど前の隠れ

20

第 2 章 関連研究

21

層をより大きく更新するようになる．
以上，周辺知識を整理した上で深層学習について述べ，本研究と関連の深い RNN につ いて詳述することで，技術的な前提知識を確認した．
次に，知識獲得の予測について述べる．

2.4 知識獲得の予測
知識獲得の予測は，生徒が対象の知識を獲得しているか否かを予測する問題である．通 常，知識を獲得しているか否かは問題回答の正誤を基に評価されるため，知識獲得の予測 のタスクは過去の生徒の問題回答履歴から次に解く問題の正誤を予測するというもので ある．
最初の定式化の事例は，1994 年に Corbett らによって報告された Knowledge Tracing [Corbett and Anderson, 1994] である．スキルの習熟学習において，領域知識をよく分析 し階層的に知識間関係を構築し，階層構造においてより水準の高い知識に着手する前に予 め獲得するべき知識が確実に獲得されるように学習体験を設計することで，ほとんどの生 徒がスキルを十分に習熟できるとする仮説 [Keller, 1968, Bloom, 1968] や，コンピュータ サイエンスの発展を受けて，予め獲得すべき知識が確実に獲得されるように生徒の知識の 獲得有無を予測するというのが主な目的であった．Knowledge Tracing における生徒とモ デルは，生徒が勉強し知識を獲得したら，モデルが生徒が獲得した知識を予測することで 生徒の獲得している知識の変化を追跡する（Knowledge Tracing），という関係になって いる．
伝統的に，知識獲得の予測には知識獲得の時系列性を重視するものと，知識間の関係性 を重視するものがある．[Corbett and Anderson, 1994] で報告された Bayesian Knowledge Tracing という手法は知識獲得の時系列性を重視するもので，問題に予めスキルを割り当 て個々のスキルに習熟過程に関する 4 つの確率変数を定義しモデル化するというもので， スキル間の関係性は考慮しないが，個々のスキルの習熟，つまり時系列性を考慮する手法 である．[Pavlik Jr et al., 2009] で報告された Performance Factor Analysis という手法は 知識間の関係性を重視するもので，個々の知識（あるいは，スキル）に関する過去の回答 の正誤を重み付けして，次の問題の正誤を予測しようというもので，Performance Factor Analysis は知識獲得の時系列性より知識間の関係性を重視する手法である．いずれの手法 も本論文と関連が深い．

21

第 2 章 関連研究

22

以降では，まず，Knowledge Tracing の定式化について述べ，Bayesian Knowledge Tracing と Performance Factor Analysis の 2 つの手法を説明し，最後に，深層学習を活用した Deep Knowledge Tracing について説明する．

2.4.1 Knowledge Tracing の定式化
Knowledge Tracing は過去の生徒の問題回答履歴から生徒が次に解く問題の正誤を予 測するというものである．生徒の時刻 t において観測された問題回答結果を qt とすれば， q1, q2, . . . , qt から時刻 t + 1 において観測される問題回答結果 qt+1 を予測するタスクと 表現できる．特に，過去の観測された問題の正誤から将来の正誤確率を算出する場合は， q1, q2, . . . , qt が観測された場合の時刻 t + 1 に着手する問題において当該生徒の回答正解 となる事後確率 p(qt+1 = correct|q1, q2, . . . , qt) を求めるタスクであるといえる．予測性能 の評価は [Yudelson et al., 2013, FALAKMASIR et al., 2015] では Accuracy6で，[Piech et al., 2015] では AUC7で行っており，目的に応じてさまざまである．本研究では，AUC によって予測性能を評価する．
なお，モデルの入力となる「問題」の粒度はさまざまである．問題への回答の結果は， その問題を回答するのに必要な知識を生徒が獲得しているか否かを評価するという点で， 知識集合を表現しており，また，その粒度もさまざまである．個々の問題をそのままモデ ルの入力次元とするものや，問題に予めタグを割り当て問題により評価される知識の粒度 をある程度整え，そのタグをモデルの入力次元とすることもある．例えば，[Piech et al., 2015] ではモデルの入力次元は演習タグもしくはスキルタグと呼ばれるものであり，演習 問題に割り当てられ，それぞれの演習問題で扱われる知識の要素を説明するものである． 通常，こうしたタグは専門家によって設計され，利用される．本論文では，個々の問題を そのままモデルの入力次元として用いる．

2.4.2 Bayesian Knowledge Tracing
Bayesian Knowledge Tracing [Corbett and Anderson, 1994]（以下，BKT）はベイズの 定理の事前確率と事後確率の関係に基づいて正解確率 p(qt+1 = correct|q1, q2, . . . , qt) をモ デリングする手法である．BKT には下記の 4 つの確率変数がある．
6正解率．予測結果全体と、答えがどれぐらい一致しているかを判断する指標．0〜1 で表され，完全な予 測時に１となる．
7正例を正しく分類した割合を縦軸に，負例を正しく分類した割合を横軸に取る ROC 曲線における，曲線 より下の面積．0〜1 で表され，完全な予測時に１となり，ランダムな予測で 0.5 付近を示す．
22

第 2 章 関連研究

23

• 初めから当該スキル理解している確率 p(L0)（もしくは p-init）
• 生徒が当該スキルを理解していない状態から理解している状態へ遷移する確率 p(T ) （もしくは p-transit）
• 生徒が当該スキルを理解しているが誤答する確率 p(S)（もしくは p-slip）
• 生徒が当該スキルを理解していないが推測で正解する確率 p(G)（もしくは p-guess）
これらの 4 つの確率変数がすべてのスキルに定義されている．つまり，スキル数を N と すれば，確率変数の合計数は 4N である．生徒 u がスキル k の問題を時刻 t に解いた場合 に正解する確率は下記の式に基づいて更新される．

p(L1)uk = p(L0)k

p(Lt|obs = correct)uk

=

p(Lt−1)uk · (1 − p(S)k) p(Lt−1)ku · (1 − p(S)k) + (1 − p(Lt−1)uk) · p(G)k

p(Lt|obs = wrong)uk

=

p(Lt−1)ku · p(S)k p(Lt−1)ku · p(S)k + (1 − p(Lt−1)ku) · (1 − p(G)k)

p(Lt)uk = p(Lt|obs)ku + (1 − p(Lt|obs)ku) · p(T )k

p(Ct)ku = p(Lt−1)ku · (1 − p(S)k) + (1 − p(Lt−1)ku) · p(G)k

(2.21) (2.22)
(2.23) (2.24) (2.25)

右上の k はスキル番号を示し，右下の u はユーザ番号を示すことに注意されたい．まず， 生徒 u が初めから当該スキル k を身につけている確率は式 2.21 の通り定義する．正解が 観測され，正しく当該スキルを身につけている確率は，式 2.22 で与えられ，不正解が観測 されたが，正しく当該スキルを身につけている確率は，式 2.23 で与えられ，それらを合わ せて，次の時刻に当該スキルを身につけている確率は，式 2.24 で与えられる．このように 定めることで，理解しているがうっかり間違ってしまう場合や， 理解していないがあて ずっぽうで正解してしまう場合を考慮できる．なお当該モデルでは，身につけたスキルの 忘却は無視している．最後に，生徒 u がスキル k の問題を時刻 t に解いた場合に正解する 確率 p(Ct)ku は式 2.25 のように算出され，この値を次の問題の正誤予測に利用する．
上記に説明したモデルの学習にはいくつかの方法が適用され報告されている．1 つは [Corbett and Anderson, 1994] にあるように隠れマルコフモデル (HMM) を用いて生成モ デルとして学習させる方法であり，1 つは [Yudelson et al., 2013] にあるように勾配法を 用いて識別モデルとして学習させる方法である．それぞれ長所と短所があるが，特に，大

23

第 2 章 関連研究

24

規模データへの適用という観点では HMM に基づいた生成モデルの手法では計算量が大 きく学習に非常に多くの時間がかかってしまうということもあり，[Yudelson et al., 2013] では勾配法に基づいた識別モデルとして学習させている．具体的には，[Yudelson et al., 2013] では，目的関数に負の対数尤度（Negative Log Likelihood）を利用し，勾配降下法 （Gradient Descent）で学習させている．

2.4.3 Performance Factor Analysis

Performance Factor Analysis [Pavlik Jr et al., 2009]（以下，PFA）も過去の生徒の問 題回答履歴から生徒が次に解く問題の正誤を予測するための手法である．しかし，知識獲 得の時系列性を考慮する BKT と異なり，知識獲得の順番を考慮せず知識間の関係性を考 慮して予測する手法である．PFA は下記のように定義される．

∑

p(i, j ∈ KCs, s, f ) = σ(βj +

(γksi,k + ρkfi,k))

k∈K C s

(2.26)

ここでは，s は事前に正答した問題回答，f は事前に誤答した問題回答，p はユーザ i が知 識 j に正答する確率，βj は知識 j の簡単さ，γk と ρk はそれぞれ知識 k の正答と誤答の重 み，si,k と fi,k はそれぞれユーザ i が知識 k に事前に正答した問題回答，事前に誤答した 問題回答である．σ はシグモイド関数，過去の各知識の正誤を重み付けしシグモイド関数 にかけ，別の問題の正誤を予測するというものである．
PFA は知識間の関係性を考慮できない場合に複数の知識がないと獲得できない知識の モデルが難しいという Bayesian Knowledge Tracing やその拡張手法の問題を解決するた めに提案された．PFA は知識間の関係性を重み付けして考慮しているが，問題回答の順番 は考慮しない．

2.4.4 Deep Knowledge Tracing
Deep Knowledge Tracing [Piech et al., 2015]（以下，DKT）は RNN を利用し Knowledge Tracing を行う手法である．2015 年 6 月に発表された．数学の問題回答ログのデータセッ トで実験され，高い性能で将来の知識獲得を予測できること，予測モデルを分析すること で知識間関係をネットワークとして抽出できることが報告された．生徒が獲得している知 識から，ある知識の獲得されやすさをそのまま予測しており，得られた知識間関係から抽

24

第 2 章 関連研究

25

出されたネットワークは知識獲得における知識構造を表現しているといえる．DKT の構 造と最適化，および知識間関係の抽出手法について順に説明していく．

構造
まず，DKT の構造について述べる．DKT の構造は伝統的な RNN の構造に基づいてい る．伝統的な RNN は入力のベクトル系列 x1, . . . , xT を出力のベクトル系列 y1, . . . , yT に 写像する．この写像は，隠れ状態 h1, . . . , hT を計算することで達成されるが，一連の写像 の過程で過去観測から得られる関連情報を将来予測のために連続的に符号化している，と みなせる．確率変数は下記の式で定義されるネットワークにより関連付けられる．

ht = f (xt, ht−1) yt = g(ht)

(2.27) (2.28)

モデルは関数 f と g によって定義されており，これらの関数 f, g には SRNN の式 2.7，2.8 や LSTM-RNN の式 2.9–2.15，GRNN の式 2.16–2.20 を利用できる．
RNN で生徒の学習行動の観測結果をモデリングするため観測結果を固定長の入力ベク トル xt の系列に変換する必要があるが，DKT ではシンプルな変換を行っている．具体的 には，生徒の学習行動の観測結果を one-hot ベクトルに符号化し xt とする，というもので ある．観測結果は演習問題と正誤の組み合わせで表現できるため，演習問題の数を M と すれば，xt の長さは 2M となる．
表 2.1: Deep Knowledge Tracing における回答ログデータと対応する入力ベクトルの例

回答ログ

入力ベクトル

ユーザ ID ログの順番 問題番号 正誤 変数名

値

A 1 1 0 x1 [0000...1000] A 2 1 1 x2 [1000...0000] A 3 2 1 x3 [0100...0000] A 4 3 0 x4 [0000...0010] A 5 3 1 x5 [0010...0000] A 6 4 1 x6 [0001...0000]

具体例を交えて説明する．例えば，演習問題の数が 4 つで，問題回答は 1 つずつしかで 25

第 2 章 関連研究

26

きないと仮定する ．M = 4 であり，xt の長さは 8 である．ある生徒が，表 2.1 の回答ロ グように問題を回答し正誤が観測されたとする．この時に，例えば，表 2.1 に記載のよう な入力ベクトルの系列となる．このようにして，回答行動の観測結果を符号化することで， どの演習問題をいつ正解もしくは不正解したのかを RNN に入力できる．
出力 yt は問題と同じ長さのベクトルで，それぞれの要素が当該生徒がそれぞれの問題 に正しく回答する確率の予測値となっている．したがって，t + 1 の回答 qt+1 の正誤予測 は t + 1 に回答される問題 qt+1 に対応する yt の要素から読み取れる．

最適化

次に，DKT の最適化について述べる．訓練時に用いられる目的関数は，モデルにおい て生徒の回答行動の観測系列の負の対数尤度（Negative Log Likelihood）である．δ(qt+1) を時刻 t + 1 にどの問題が回答されたかの one-hot ベクトルとし，at+1 を時刻 t + 1 に当該 問題で正答したか否か（1 か 0）とし，l を交差エントロピー誤差とすれば，当該予測結果 に対する損失関数は l(yT δ(qt+1), at+1) であり，生徒一人の損失関数は下記の式で与えら れる．

∑

L=

l(ytT δ(qt+1), at+1)

t

(2.29)

学習時はミニバッチごとに確率的勾配降下法で目的関数を最小化する．[Piech et al., 2015] では，モデル学習時には過学習を防ぐため yt への入力としての ht には dropout [Srivastava et al., 2014] を適用している（ht+1 の方向には dropout を適用しない）．また，系列方向 の誤差逆伝搬 [Werbos, 1990] において勾配が爆発するのを防ぐため，閾値以上のノルム の勾配は [Pascanu et al., 2013] にしたがい，制約を設けている．

知識間関係抽出法
次に，DKT のモデルを利用した知識間関係（あるいは，問題間関係）抽出法について 述べる．DKT のモデルは，従来ではよく人間の専門家が行っていたデータの潜在的な構 造や概念を発見するタスクに応用できる．問題 i と j のすべての有向ペアのうち下記の条 件を満たすものに対して下記の影響度 Jij を割り当てる．

26

第 2 章 関連研究

27

条件 有向ペア (i, j) について，問題 i が出現した後に残りの問題系列の中で問題 j が出現 する系列数が問題 i が出現する問題系列数全体の V % 以上であること．

影響度

Jij

=

∑y(j|i) k y(j|k)

ここでは，y(j|i) は，ある生徒が最初に問題 i に正答した場合に，RNN によって割り当て られる次の時刻に問題 j に正答する確率である．[Piech et al., 2015] では，問題間影響行 列からのネットワーク抽出には，V = 1 を用いた．また，ネットワークの可視化に際して は，影響度が 0.1 以上であればエッジを引くというようにしてネットワークを構築した．
さらに，[Piech et al., 2015] は，得られたネットワークは，単に生徒の問題 (i, j) 間の遷 移率から構築したネットワークや問題 i の正解が観測された後に問題 j の正解が観測され る条件付き確率から構築したネットワークよりよく知識間関係を捉えていることを指摘し ている．
こうして得られた行列 J は，問題 i で評価される知識が既に獲得されている場合に，問 題 j で評価される知識の獲得されやすさを表現しており，J は知識間関係行列であるとい え，この知識間関係行列から構築したネットワークは知識獲得における知識構造を表現し ていると考えられる．
2.4.5 知識獲得予測の手法における DKT の最適性
ここまで知識獲得予測の様々な手法について述べたが，DKT を拡張する手法が，本研 究の目的を達成する上で最適な手法であることを，以下の二点に基づいて説明する．
1. 知識間の関係性は，知識獲得予測の文脈において，定量的に検証されて抽出される べきこと，
2. 知識獲得予測は，複数の知識間の影響関係や，知識獲得の時系列性を考慮して行わ れるべきこと，
まず，１について述べる．知識間の関係性は，知識獲得予測の過程で抽出されるものと， そうでないものがある．後者については，専門家が作成するという手法や，テキスト解析 により概念関係ネットワークを構築するという手法 [Chen et al., 2008] がある．しかし，
27

第 2 章 関連研究

28

これらの手法は，専門家や研究者が立てた仮説に基づいた定性的なものであり，実際の生 徒の学習過程をよく説明するものであるという定量的な根拠はない．
問題回答正誤の分析により知識の構造化を行う方法 [参考文献引用] も，2 つの問題 i と j の間で問題 i が正解後と不正解後の問題 j の正解率の差と着手順序を基に知識を構造化 するが，この手法は 2 つの問題 i と j の関係性のみを考慮しており，他の問題との関係性 は独立だと見なされている．得られた知識間関係は 2 つの知識の間についてのものを線形 に合算したものであり，複雑で密接に関係している複数の知識の獲得順序や影響関係を捉 えているものではない．
一方で，知識間の関係性の抽出を，知識獲得を予測する過程で行うものは，生徒の知識 状態と行動を元に，知識の獲得を予測しているため，生徒の学習過程を反映した知識間関 係を表現している可能性が高い．
したがって，知識間関係を定量的に抽出する手法としては，知識獲得を予測する過程で 知識間関係を抽出する手法に絞る．
次に，２について述べる．知識獲得予測には，複数の知識の影響関係や知識獲得の時系 列性を考慮するものと，そうでないものがある．後者については，複数の知識を独立なも のとして，それらの状態の遷移を定義する Bayesian Knowledge Tracing(BKT) の手法や， 過去の回答の結果を１つにまとめて定義する Performance Factor Analysis(PFA) の手法 などがある．しかし，BKT の手法は，複数の知識を独立なものとして捉えるため，複数 の知識からなる複雑な知識状態を捉えきれず，また，PFA の手法は，直前の回答も十分な 時間が立った後の回答も，一つの過去の回答として捉えるため，生徒の時間に沿った知識 獲得の状態を，現実的に捉えきれていない．
一方，DKT は，時系列に沿って RNN の隠れ層を更新することにより，複数の知識間 の影響関係や，知識獲得の時系列性を考慮して知識獲得を予測しているので，より現実に 沿った知識間関係を抽出できる可能性が高い．
現に [Piech et al., 2015] で，既に DKT によって知識間関係を抽出できることが報告さ れており，複雑で密接に関係している複数の知識の獲得順序や影響関係を捉えている可能 性が高く，DKT を利用することが最適であると考えられる．
また，PFA と DKT のいずれの手法も知識間関係を考慮して予測に利用しているが，DKT の方が有効性が高いと考える．なぜなら，[Piech et al., 2015] では言及されていなかった

28

第 2 章 関連研究

29

が，DKT は PFA の拡張になっているためである．DKT は RNN を利用しており，

ht = f (xt, ht−1) pt = σ(ht−1 · Whp + bp)

で与えられる．一方で PFA は

∑

p(i, j ∈ KCs, s, f ) = σ(βj +

(γksi,k + ρkfi,k))

k∈K C s

で与えられる．したがって，

(2.30) (2.31)
(2.32)

f (xt, ht−1) = xt + ht−1 h0 = [0, 0, · · · , 0]

(2.33) (2.34)

とすると，ht がこれまでの各問題についての正答回数を表現するベクトルと各問題につ いての誤答回数を表現するベクトルを結合したベクトルになるが，これは，ベクトル s と f を結合したものと同じである．したがって，PFA は DKT 内部の RNN の繰り返しの部 分を表現する関数 f を式 2.33 にした特殊なケースであり，DKT は PFA の拡張になって いる．

以上，知識獲得の予測研究について述べ，本研究の素地となっている研究について整理 した．次に，次元削減手法について述べる．

2.5 次元削減手法
高次元のデータから低次元の特徴表現を抽出する，次元削減手法について述べる．本研 究が目的とする知識分類表現の抽出は，一般的な次元削減の手法を拡張したものであり， 一般的な次元削減手法について説明することで，基礎となる知識や本研究との差分を明確 にすることを目的とする．
一般に，機械学習や統計においては，扱うデータの次元が大きい場合に，次元削減を 行うことが多い．これは，データの次元が大きすぎることにより，データのサンプル数 に対してモデルが複雑化してしまい，認識精度が悪くなる「次元の呪い」[Bellman and Corporation, 1957, Friedman, 1997] という現象を回避する目的の他，可読性を高めるこ とにより，人間が解釈しやくすることなどを目的としている．知識獲得予測において用い
29

第 2 章 関連研究

30

られる知識分類も，分類のなされていない生の問題は次元数が大きく，そのままでは人間 が解釈したり教育に用いることが困難なため，内容や難易度の類似度など，一定の尺度に 従って，人間の手により次元削減が行われた例である．本研究ではこうした次元削減を， 人間の手ではなく深層学習によって行うことで，最適化することを目的としている．
本節では，機械学習が現れる以前から一般的に使用されていた次元削減手法の代表であ る Principal Component Analysis や，ニューラルネットワークを活用した次元削減手法 である Autoencoder，そして，深層学習の過程で行われる Embedding と呼ばれる埋め込 み手法を取り上げ，次元削減の手法について概観する．

2.5.1 Principal Component Analysis

Principal Component Analysis は，日本語では主成分分析と訳される．相関のある多数 の変数の中から，分散の大きい変数をデータ全体を説明する上で重要な「主成分」と見な し，順にそれまでの主成分と直行するように主成分を定める変換を繰り返し行うことで， 変数間の相関をなくし，重要度の高い主成分のみを採用し，次元削減を行う手法である．
その由来は古く，1901 年に力学の分野において初めて導入された [Pearson, 1901] こと をきっかけに，以後，経済学や統計学，機械学習などの分野で幅広く利用されてきた [Wold et al., 1987, Ku et al., 1995]．
一般的なアルゴリズムを以下に示す． 元のデータを X，データの次元を M ，変換後の次元を N ，とすると、
1. データの共分散行列を求める．

2. 共分散行列の固有値と固有ベクトルを求める．

3. 固有値の大きい順に，対応する固有ベクトルを並べ替え，N 個の固有ベクトルを並 べた行列 P を作る．

4. データからその平均ベクトルを引いたデータを Xbar とし，以下の式に基づいてデー タを変換する．

Xpca = XbarP

(2.35)

主成分分析には様々な拡張がある [Sch¨olkopf et al., 1997, Tipping and Bishop, 1999]

が，その根底にある数学的な意味は，固有値問題を解くことにある．固有値問題とは、あ

る行列 A について、

Ax = λx

(2.36)

30

第 2 章 関連研究 となるような固有値 λ と固有ベクトル x を求めることである．

31

2.5.2 Autoencoder
Autoencoder は，日本語では自己符号化器と訳される．1980 年代から考案されていた が，2006 年の [Hinton and Salakhutdinov, 2006] らの研究によって広く普及した．様々な 種類のものが考案されているが，基本的な概念は，ニューラルネットワークに入力された データを一旦低次元で表現し，その低次元表現から再び入力である自身を再現するように 学習させることで，データの特徴を適切に表す低次元表現を獲得することを目的としてい る．教師データが存在しないが，自身を教師データとして行う教師あり学習のような形を 取っており，教師あり学習と教師なし学習の中間に位置するような概念として理解される こともある．
具体的な定式化について述べる．まず，入力層 x ∈ Rd に対して，以下の式によって隠 れ層 h と復元層 xr を定義する．

h = fθ(x) = s(Wx + b) ∈ Rdh xr = gθ (h) = sr(W h + b′) ∈ Rd

(2.37) (2.38)

ここで，θ = (W, b), θ′ = (W′, b′) は学習されるパラメータであり，s, sr は活性化関数

である．こうして定義された復元層 xr が入力層 x にできるだけ近づくように，訓練デー

タ D = x1, · · · , xn に対する損失関数の平均値を最小化する過程で，パラメータ θ, θ′ を学

習する．

1 ∑n

min θ,θ′ n

L(xi, g(f (xi)))

i=1

(2.39)

式 2.39 で表される損失関数は，一般に再構成誤差 (Reconstruction Error) と呼ばれる．

損失関数 L は，入力がバイナリ値ならば交差エントロピー誤差，実数値ならば二乗誤差を

用いるのが一般的であり，本研究においては，入力は問題回答の正誤を意味するバイナリ

値なので，交差エントロピー誤差を用いる．

活性化関数が恒等写像，つまり活性化関数が存在せず，損失関数が二乗誤差の場合は

PCA と等価になることが知られており [参考文献]，Autoencoder は，ニューラルネット

ワークの構造と非線形の活性化関数を用いることにより，PCA の拡張を行っていると見

なすことができる．

このように，次元削減を非線形に行うことができる Autoencoder だが，深層学習が発達

31

第 2 章 関連研究

32

した今日では，深層学習モデルの各ユニットに良い初期値を与えるための事前学習として 利用されることが多い [Erhan et al., 2010]．より頑健な特徴表現を獲得させるためにデー タにノイズを加える Denoising Autoencoder [Vincent et al., 2008] や，潜在的な特徴表現 の分布に特定の確率分布が存在すると仮定する Variational Autoencoder [Kingma et al., 2014] など，様々な工夫が考案されている．

2.5.3 Embedding
Embedding は，日本語では埋め込みと訳される．一般に，ニューラルネットワークにお いて入力データの次元数が大きい場合，そのままデータを入力すると，データの特徴的な 表現について学習がしにくくなることがある．そのため，一度次元数の少ない層へ埋め込 みを行い，データのより特徴的な表現を抽出した上で学習を進めることで，精度が向上す る場合があることが知られている [参考文献？]．
学習後のモデルにおける埋め込み層は，入力データを低次元で表現する上で最適な特徴 表現となっている可能性が高く，この表現を抽出することでデータの特徴を保ったまま次 元削減を行うことができる．
最後に，以上の関連研究を踏まえて，本論文で使用する類似の用語について定義を明確 にする．

2.6 用語の定義
本節では，本論文で用いられる類似した用語の定義を行い，意味上の違いを明確にする ことで，以降の手法の説明を含めた本論文の展開を明確にすることを目的とする．
2.6.1 知識獲得予測と回答正誤予測
一般に Knowledge Tracing と呼ばれ，本研究が問題提起を行っているのは「知識獲得予 測」である．しかし，本研究は従来の「知識獲得予測」が「知識」の定義として用いてい る知識分類を所与のものとせず，問題の回答ログのみを入力に用いて，その回答の正誤予 測を最適化する過程で適切な知識分類を学習することを目的としている．よって，知識分 類を学習する段階では，まだ「知識」の定義はなされていないため，この段階では正確性 を期すために「回答正誤予測」という単語を用いる．

32

第 2 章 関連研究

33

一方，学習によって得た知識分類を用いて既存の知識分類と比較を行う際には，既に「知 識」の定義がなされているため，一般的な「知識獲得予測」の単語を用いる．

2.6.2 知識分類と知識タグ
本研究で扱うデータセットには，既存の「知識分類」が存在する．問題に対して事前に 分割されている知識ごとの分類や，そのような状況を「知識分類」と呼び，生徒が回答す る各問題に紐づき，その内容を指し示す１つ１つの具体的なタグのことを「知識タグ」と 呼ぶ．指し示している状況は似ているが，問題全体が事前に分類されている状況を意図す るときは「知識分類」の単語を，より個別の問題に紐づく具体的なタグを意図するときは 「知識タグ」の単語を使用する．
以上，関連研究について述べ，本研究の学術的位置付けや周辺概念を俯瞰した． 次に，分析手法について述べる．

33

第 3 章 分析手法
本章では，分析手法について説明する． まず，分析手法全体の流れを概説し，手法全体が３つの要素から構成されることを述べ たのち，各要素について詳述する．
3.1 分析手法全体の流れ
まず，分析手法全体の流れを説明する．本研究の手法は，データセットの作成という事 前処理と，提案手法による知識分類の学習，学習された知識分類の知識獲得予測性能に関 する検証，学習された知識分類の性質に関する比較分析という３つの分析から構成される．
まず，データセットの作成について述べる．本論文が扱う，生徒の知識獲得予測におい て，生徒が知識を獲得しているか否かの評価には，生徒の問題回答ログデータを対象デー タセットとして用いる．その際，比較検証に用いるため，知識獲得の予測に利用できる複 数のデータセットを利用し，また，本研究に適用するために，幾つかの条件に基づいて対 象データを抽出する．
次に，３つの分析の手法について述べる． まず，知識分類の学習について述べる．知識獲得の予測性を最適化するような知識分類 を抽出するには，問題と知識分類の最適な関係性を深層学習によって学習する必要がある． そのために，問題を知識分類に変換する写像関数をパラメータ化し，回答正誤予測の最適 化の過程で同時に学習する．このモデル構造は，Deep Knowledge Tracing のモデルを拡 張して設計する． 次に，知識分類の性能検証について述べる．学習された知識分類を用いることにより， 既存の知識タグや一般的な次元削減によって得た知識タグを用いる場合より，高い精度で 知識獲得が予測できることを示すことで，本手法により知識獲得の予測性を最適化する知 識分類が得られたことを定量的に示す． 最後に，知識分類の性質分析について述べる．学習された知識分類の性質や構造を，既 存の知識分類の構造と比較することにより，その性質を定性的に検証する．
34

第 3 章 分析手法

35

以降では，データセットの作成，提案手法による知識分類の学習，学習された知識分類 の知識獲得予測性能に関する検証，学習された知識分類の性質に関する比較分析について 順に詳述する．

3.2 データセットの作成
本研究においては，生徒の問題回答ログデータをデータセットとして用いる．生徒の問 題への回答結果は，その問題が問う知識を，生徒が既に獲得しているか否かを表現してい ると捉えることができるため，回答結果が正解であれば，該当の知識を既に獲得しており， 回答結果が不正解であれば，該当の知識を未だ獲得していないと捉えることができるから である．
問題回答ログデータから作成するデータセットは，下記の要件を満たす必要がある．
1. データセットが大規模であること．
2. 比較検証できるデータセットが複数存在すること．
3. 既存の知識分類が存在すること．
まず，データセットが大規模である必要について説明する．一般に，深層学習は大量 のデータを元に特徴的な表現を抽出するため，深層学習モデルを十分に学習させるには， 大規模なデータが必要である．これは，本研究で用いる，深層学習モデルの一つである Recurrent Neural Network(RNN) を活用する Deep Knowledge Tracing についても同様 である [Piech et al., 2015]．したがって，大規模なデータを有することがデータセットの 要件の一つとなる．また，深層学習によって知識獲得の予測を行う本研究においては，単 に全体のデータ数が多いだけでなく，分析対象となる個別の問題や生徒について，十分な データ数を確保できることが重要である．例えば，一度しか回答されていない問題につい ては，その問題の正答や誤答によって，生徒の知識状態がどのように変化するかが観察で きないため，分析に適していない．また，十分な数の生徒のデータがないと，特定の生徒 の学習傾向が強く反映され，実験結果の一般性が損なわれる可能性がある．
次に，比較検証できるデータセットが複数存在する必要について説明する．本研究で 用いる，問題回答ログからなるデータセットは，そのデータセットが提供されるプラット フォームにより，問題を回答している集団や，扱っている教科，内容のレベルなどが異な る．特定のデータセットのみに対して得られた結果は，そのデータセットの環境において のみ有効である可能性があり，一般性のある結果や知見が得られたとは言いにくい．その
35

第 3 章 分析手法

36

ため，本研究では，教科を数学に絞った上で，複数のプラットフォームにおける問題回答 ログから複数のデータセットを作成することで，手法の一般性を検証する．なお，数学に 限らない，他教科への適用可能性については，第 6 章で考察する．
最後に，既存の知識分類が存在する必要について説明する．本研究では．現在の一般的 な知識獲得予測に用いられている知識分類は，人間の複雑な知識獲得の過程を表現する上 で最適化された表現ではない，という仮定に立ち，問題回答ログのみを利用して，より最 適化された知識分類を抽出することを目的としている．抽出された知識分類の妥当性を 検証するには，既存の知識分類と比較することが必要であり，そのため，分析対象となる データセットは，既存の知識分類が存在する必要がある．

3.3 提案手法による知識分類の学習
本節では，提案手法による知識分類の学習について述べる．本研究では，知識獲得予測 を最適化する知識分類を学習するために，問題を知識分類に変換する関数をパラメータ化 し，Knoeledge Tracing の最適化の過程で同時に学習する．なお，以降では，この提案手 法のモデルを「知識分類学習モデル」と呼ぶ．この知識分類学習モデルの構造は，Deep Knowledge Tracing（以下，DKT）を元に設計されているため，まず，DKT を拡張する 方法について述べる．
3.3.1 DKT の拡張による写像関数の学習
問題を知識分類に変換する写像関数をパラメータ化し，Knoeledge Tracing の最適化の 過程で同時に学習するために，本研究では，既存の DKT の構造を拡張した「知識分類学 習モデル」を設計する．DKT のモデルを拡張する方法を，以下の 3 つの要素に分けて説 明する．
1. 入力データの粒度
2. モデル全体の構造
3. 最適化手法

36

第 3 章 分析手法

37

入力データの粒度
まず，既存の DKT と大きく異なる点として，モデルに入力されるデータの粒度の違い について述べる．DKT のモデルにおいては，使用するデータセットは生徒の問題回答ロ グデータであるが，モデルへの入力は，事前に定義された知識分類に基づいて，知識タグ に落とし込まれ，どの知識タグが紐づく問題に回答したかが入力される．これは，既存の 知識分類の範囲内で生徒がどのように知識を獲得していくかを予測することを前提にして いるためであるが，本研究においては，そもそもの問題と知識分類の関係性を最適化する ことを目的とするため，この入力は適さない．よって本研究では，モデルへの入力は問題 に対する回答のままにとどめ，生徒が次にどの問題に正解するかを予測する過程において， 深層学習モデル自身に最適な知識分類方法を判断させることで，最終的に，知識獲得予測 を最適化するような知識分類を学習させる．こうして学習された知識分類は，結果的に知 識獲得の文脈で最適化されている可能性が高く，既存の知識タグと比較することで，その 性能や性質を解釈することができる．

モデル全体の構造
次に，具体的なモデル全体の構造の拡張について述べる．まず，DKT では入力が RNN の隠れ層へ直接伝達されるのに対し，知識分類学習モデルは，まず入力層の問題空間 X か ら，抽出目的の知識タグ空間 U への写像を行う．ここで言う知識タグ空間とは，既存の知 識分類と同じ次元数に設定された空間で，問題回答の正誤の情報を低次元の空間で表すこ とを目的としている．
具体的には，まず，問題数を M とした場合，正答と誤答を区別するために，モデルへ の入力ベクトル xt の長さは ２ M となる．第二層の知識タグ空間の次元数は，既存の知識 分類と同じ次元数に揃え，事前に N と定義する．そして，M 次元の問題空間 X から N 次元の知識タグ空間 U へ変換する写像関数 P を，以下の式により定義する．

P(xt) = σ(Wxuxt + bu)

(3.1)

ここで，xt は長さ M の問題空間ベクトルを指し，Wxu は M × N の大きさの重み行列を 指し，bu は長さ N のバイアス項を指し，σ は 1/(1 + e−x) で定義されるシグモイド関数 を指す．訓練時には，Wxu，bu を学習する．
このように定義される写像関数 P を，xt の前半の正答部分と後半の誤答部分に別々に 適用し，連結することによって，正誤の情報を区別したまま，長さ ２ N の知識タグ空間

37

第 3 章 分析手法

38

ベクトル ut が生成できる．

ut = [P(xt positive), P(xt negative)]

(3.2)

ここで，xt positive，xt negative はそれぞれ問題回答の正答と誤答を表す長さ M のベクトル である．
こうして得られた知識タグ空間ベクトル ut は，一般的な RNN 同様，隠れ層を経由して 時系列情報を反映した後，再び長さ ２ N の知識タグ空間ベクトル vt となる．

ht = tanh(Wuhut + Whhht−1 + bh) vt = σ(Whvht + bv)

(3.3) (3.4)

ここで，ht は時刻 t の隠れ層を指し，Wuh，Whh はそれぞれ重み行列を指し，bh，bv はそ れぞれバイアス項を指し，tanh は (ex − e−x)/(ex + e−x) で定義される Hyperbolic Tangent 関数を指す．訓練時には，Wuh，Whh，bh，bv を学習する．この知識タグ空間ベクトル vt は，時刻 ｔ までの回答情報を反映した，生徒の知識タグ空間における知識状態を表し ていると言える．
最終的に，この知識タグ空間ベクトル vt から，予測結果として M 次元の問題回答予測 ベクトル yt を算出する．

yt = σ(Wvyvt + by)

(3.5)

ここで，Wvy は重み行列を指し，by はバイアス項を指す．学習時には Wvy，by を学習 する．
yt は０から１の間の値を取り，次の時刻 t + 1 において各問題に正答する確率を表して おり，既存の DKT と同じ予測表現となっている．
以上のようなモデル構造上の拡張をまとめた図を図 3.1 に表す．橙色の層は知識タグ空 間を，青色の層は問題空間を表し，層が左右で二つに区切られている部分は前半・後半が それぞれ正答・誤答の情報を表現している．問題空間から知識タグ空間への写像関数 P は P = σ(Wx + b) で表され．正答部分と誤答部分に同じ関数が適用される．
この拡張の目的は，M 次元の問題空間 X から N 次元の知識タグ空間 U へ変換する写 像関数 P をパラメータ化し，回答正誤予測の最適化を行う過程で学習することにある．

38

第 3 章 分析手法

39

図 3.1: モデル構造上の拡張

最適化手法

既存の DKT における最適化手法は，時刻 t の出力である問題回答予測ベクトル yt と， 実際の時刻 t + 1 の問題回答ベクトル δ˜(qt+1) の誤差を損失関数として，これを最小化す るものである．at+1 を時刻 t + 1 に対応する問題で正答したか否か（1 か 0）のベクトルと すれば，

∑mt log(p1 × p2 × · · · × pmt) = log(pk)
k

(3.6)

であることから，損失関数は

∑ Lp = l(ytT δ˜(qt+1), at+1)
t

(3.7)

である．この損失関数を，回答正誤予測に関する損失関数 Lp とする． 本研究では，この回答正誤予測に関する損失関数に加え，式 2.37–2.39 で表される再構
成誤差も損失関数として導入する．ここで，式 2.37，2.38 におけるパラメータについては， 本研究では，W，b が，入力層で問題空間から知識タグ空間へ写像する際に用いる Wxu， bu であり，W′，b′ が，出力層で知識タグ空間から問題空間へ復元する際に用いる Wvy， by である．この入力の再構成誤差の損失関数を Lr とすると，結果的に，モデル全体の損

39

第 3 章 分析手法

40

失関数 L は以下の式によって定められ，この損失関数を最小化するようにモデルが最適化 される．

L = Lp + Lr

(3.8)

ここで，再構成誤差を損失関数に導入する理由について述べる．まず，一般的に再構成 誤差を用いる Autoencoder は，深層学習モデルに良い初期値を与えるための事前学習の ための仕組みとして用いられるが，本研究では，この Autoencoder の構造を DKT と同時 に学習させるようにモデルに組み込んでいる．Autoencoder の構造を，事前学習ではなく 普通の学習モデルに組み込むことは，必ずしも精度向上につながるわけではないため，一 般的ではなく，単純に低次元ベクトルへの埋め込み (Embedding) のみに留まることが多 い．本研究では，一部，深層学習によって知識分類を学習するには十分とはいえないログ 数の問題が存在しており，データの特徴を把握しきれない未学習 (underﬁtting) の状況に 陥る可能性があることを確認した．そうした学習不足への対策として，モデルの学習を矯 正し，適切に学習を進めさせる正則化項として再構成誤差を導入している．このように， データ数が不足する場合に，モデルがより適切に学習を進めるように様々な正則化項を設 ける手法は一般的であり，有効な手法とされている．学習される知識分類の性質への影響 についても検討した結果，Autoencoder の構造は，問題の正答・誤答と知識タグの理解状 態は相互に変換できるはずだという教育学的な文脈との整合性と合致し，知識分類の性質 を損ねないと判断したため，適用している

3.3.2 写像関数の離散化による知識分類の作成
3.3.1 の知識分類学習モデルで得られた写像関数 P から，実際に知識分類を作成して問 題をタグ付けし，既存の知識タグと比較する手法について説明する．
知識獲得予測の最適化の過程で得られた写像関数 P は，M 次元の問題空間を N 次元の 知識タグ空間に写像する，M xN の大きさの行列として表現されている．この行列は０か ら１の値を取る連続値表現であるため，そのままでは問題がどの知識タグに紐づくかを特 定することはできない．そのため，何らかの方法で，この行列を０か１の２値を取る離散 表現に改める必要がある．
離散化の方法としては，各問題に対して最も関係性の強い知識タグのみをタグとする方 法や，行列全体で特定の閾値を定め，その閾値を超えたものをタグとする方法，両者を組 み合わせる方法など，様々な方法が考えられる．各方法によって，抽出された知識タグの
40

第 3 章 分析手法

41

性質の異なる解釈をすることが可能だが，本研究では，知識獲得予測に適用した際に最も 良い精度で予測できる分類を，知識獲得の遷移を最もよく説明する分類として見なして利 用し，その性質も解釈する．まず，各手法において得られた写像行列を，以下の条件に基 づいて離散化し，DKT において最も高い精度を発揮したものをその手法による精度の上 界として採用する．
1. 各問題の写像ベクトルにおいて，値が上位 X 位以内のタグを１とする
2. 各問題の写像ベクトルにおいて，値が閾値 Y 以上のタグを１とする
3. 各問題の写像ベクトルにおいて，1 でない要素を 0 にする

3.4 学習された知識分類の知識獲得予測性能に関する検証
次に，抽出された知識分類 (以下，抽出タグ) の知識獲得予測における性能を，比較検証 する手法について述べる．まず，抽出タグが，知識獲得の予測性を最適化する表現となっ ていることを示すために，抽出タグに基づいた回答ベクトルを一般的な DKT に入力する ことで，既存の知識タグ (以下，既存タグ) を用いた場合よりも精度が向上することを確認 する．また，この精度向上が，回答正誤予測の最適化の過程で知識分類を作成したことに 起因することを示すため，知識獲得予測と無関係の，一般的な事前学習の Autoencoder で 作成した知識タグを用いた場合とも比較を行う．これは，本手法による知識分類の作成が，
• 人間の可読性を目的として人間によって設計された分類か否か
• 知識獲得予測の文脈で最適化された分類か否か
という二つの観点において新規性を有することを受け，性能の変化が何に起因してもたら されたのかという，差分をより明確にするために行う検証である．

3.5 学習された知識分類の性質に関する比較分析
最後に，こうした知識獲得予測の精度向上という定量的な検証に加え，抽出タグの性質 について，既存タグとの比較を通して，定性的な分析も行う．これは，知識獲得予測にお いて最適化されている知識分類表現の性質を解釈し，知見を得ることで，教育における実 用性を考察することにつなげることが目的である．

41

第 3 章 分析手法

42

まず，抽出タグと既存タグについて，回答される頻度や問題との紐付き方の分布に着目 し，知識獲得予測の精度を向上させる要因を，データ構造という側面から分析する．また， 抽出タグと既存タグが，内容の面においてどのような関係性があり，抽出タグによって既 存タグがどのように再配置されたのかを検証する．
抽出タグと既存タグの内容を比較する方法として，まず，抽出タグが紐づく問題と，既 存タグが紐づく問題から，抽出タグと既存タグの共起行列を作成する．この行列は，各抽 出タグと既存タグとの関係性の近さを表現した行列と捉えることができるが，各抽出タグ の特徴をより明確に捉えるために，TF-IDF 法を用いて，特徴の重み付けを行う．TF-IDF 法は，元々文書の分類に用いられる手法で，複数の文書がある時に，各文書を特徴づける 単語を特定することを目的にしている．具体的には，まず，各文書内での，各単語の出現 頻度 (Term Frequency，以下 T F ) を求める．T F は，文書内に多く出現する単語ほど重要 だ，という考えに基づいた指標であるが，T F のみだと，例えば助詞や助動詞と言った，い くつもの文書で横断的に使われている単語の重要度が高くなってしまうため，各単語が全 文書の内いくつの文書にあらわれているかという制約 (Inverse Document Frequency，以 下 IDF ) を設けて，一般的な単語の影響を除外する．単語 i の文書 j における T Fi,j，単 語 i の IDFi と，それらを用いた TF-IDF 値 (T F IDFi,j) は以下の式で表される．

T Fi,j

=

∑ni,j k nk,j

|D|

I DFi

=

log |{d : d ∋ ti}|

T F IDFi,j = T Fi,j · IDFi

(3.9)
(3.10) (3.11)

ni,j は単語 i の文書 j における出現回数を指し，∑k nk,j は文書 j におけるすべての単語の 出現回数の和を指し，|D| は総文書数を指し，|{d : d ∋ ti}| は単語 i を含む文書数を指す．
さて，今回の抽出タグの特徴付けにおいては，各既存タグの成分が単語にあたり，抽出 タグ１つ１つが文書にあたる．複数の抽出タグに共通して現れる既存タグの成分ほど，特 徴づけにおける重みが小さくなり，特定の抽出タグのみに現れる既存タグの成分ほど，特 徴づけにおける重みが大きくなる．この手法により各抽出タグの特徴付けを行い，各抽出 タグにおいて特徴的な既存タグの成分を特定する．そして，既存の DKT のネットワーク分 析手法に則り，既存タグをノードとする知識間影響ネットワークを作成した上で，各抽出 タグのノードを追加で作成し，先程特定した特徴度に応じて既存タグのノードに対しエッ ジを引くことで，抽出タグと既存タグの関係性を可視化し比較する．
以上の分析手法全体の流れを，図 3.2 にまとめた．

42

第 3 章 分析手法

43

図 3.2: 分析手法全体の流れ
以上，分析手法について述べた．次章では，実験で利用するデータセットについて述 べる．

43

第 4 章 データセット

本章では，実験で用いるデータセットについて述べる． 本研究では，オンライン教育サービスにおける，生徒の問題回答ログをデータとして用 いる．その際，比較検証のため，教科を数学に絞った上で，２つのデータセットを用意す る．いずれのデータセットも，前章で述べたデータセットの要件の一つである，既存の知 識分類を有するという要件を満たしている．以下では，各データセットについて概説した 後，本研究に適用するためのデータの抽出方法について述べる．
4.1 ASSISTments 2009-2010
本データセットは，オンライン学習サービスの「ASSISTments1」における，生徒の問 題回答ログから生成されている．まず，ASSISTments のサービスについて概説した後，本 研究で用いるデータセットについて説明する．その際，本研究に適用する際に問題となる データの性質に言及した上で，その問題点を解消するためのデータの抽出方法を述べる．
4.1.1 ASSISTments のサービス
ASSISTments は Intelligent Tutoring System(ITS) の一つで，2017 年１月現在で，14 の国と 42 の州において利用されている．数学や科学，英語や社会と言った科目をカバーし ており，レベルは日本における小学生から高校生まで様々である．基本的な仕組みは，シ ステムが生徒に課した問題を生徒が回答し，その結果をシステムが自動的に採点，間違え た場合はヒントを出すなどして，生徒の知識獲得を促すもので，教師や親がその回答結果 や統計情報から生徒の習熟度を確認できること，また，必要があれば，システム上で提供 されている教材を自由に編集して，新たな問題を作成できる柔軟性の高さなどから，様々 な教育機関やオンライン教育サービス上で活用されている．

1https://www.assIstments.org/

44

第 4 章 データセット

45

4.1.2 対象データセット
本研究で用いるデータセットは，「ASSISTments 2009-2010」と呼ばれる，ASSISTments における，生徒の 2009 年から 2010 年の間の数学の問題回答ログの内，「skill builder」2 と 呼ばれるデータセットである．
元々，「ASSISTments 2009-2010」には「skill builder」と「non skill builder」という， 系統の異なる２つのデータセットが含まれている．「skill builder」は，生徒に知識を段階 的に身につかせることを目的にした系統で，ある知識を問う問題に生徒が連続で正答でき た場合に該当の知識を習得したものとみなし，次に進ませるというものである．日本の教 育現場で言えば，授業ごとの小テストに近いものといえる．一方，「non skill builder」は， 生徒がそれまで学んできたことを正しく身につけられているかを確認することを目的にし た系統で，さまざまな知識を問う問題を，まとめて生徒に課すものである．日本の教育現 場で言えば，期末テストに近いものといえる．
このような性質から，「skill builder」のデータセットの方が，生徒の知識獲得過程の細か な推移を観察する上で適しているため，Deep Knowledge Tracing [Piech et al., 2015] を始 めとする Knowledge Tracing に関する多くの研究で利用されるデータセットであり，本研 究でも同様に，「skill builder」のデータセットを用いる．生徒が解く問題 (problem) には， １つ以上の知識タグ (skill) が紐付いている．「skill builder」のデータセットには，4,217 人 の生徒の，124 の知識タグが紐づく 26,688 の問題に対する，401,756 の回答ログが含まれ ている．なお，「skill builder」のデータセットは，行の重複などによって大幅な不備が指 摘されたため訂正版が提供されており，それ以前の研究結果は信憑性が低い．本研究では， 訂正後のデータセットを用いている．

4.1.3 データの抽出
本データセットは，本研究に適用する上で以下の３つの問題を抱えている．順に，各問 題と対策について述べる．
まず，問題 (problem) の中には複数の知識タグ (skill) が紐付いているものがあるが，そ うした問題が回答された場合には，知識タグの数だけ，ログが別々に作成されている．こ れは，見かけ上のログ数 (401,756) が，実際に回答された回数より多くなっているだけで なく，同時に回答された問題や知識タグが，別々のタイミングで回答されたとみなされる
2https://sites.google.com/site/assistmentsdata/home/assistment-2009-2010-data/ skill-builder-data-2009-2010
45

第 4 章 データセット

46

危険性があり，この前提を考慮しない Knowledge Tracing は不適切であることが指摘され ている [Xiong et al., 2016b]．このため，重複している行を一つにまとめる作業が必要で ある．
次に，既存の知識タグ (skill) についても，存在はするものの，名前が割り当てられてい ないものが存在し，これらは抽出された知識タグと比較することが不可能なため，除外す る必要がある．
また，本データセットは，全体で見ると十分大規模であるといえるが，個別の問題に関 して言えば，ほとんど回答されていない問題が，全体に対して大きな割合を占めている (図 4.1a を参照)．十分なログ数を保有しない問題は，大規模データから深層学習によって知 識分類を学習するという本研究の目的を満たさないため，ログ数について一定の閾値を設 けてデータセットを切り分けることにより，適切なデータを抽出する必要がある．
以上より，本研究では，元のデータセットから，以下の方法で分析対象とするデータを 抽出している．
1. 同時回答を意味する重複行を一つにまとめる．
2. 名前が割り当てられている知識タグを持つ問題の回答ログのみを抽出する．
3. 2 のうち，最低 30 回以上回答されている問題の回答ログのみを抽出する．
4. 3 に含まれる問題を，最低 2 回以上回答している生徒に関するログを抽出する．
なお，3 の 30 回以上という具体的な数字は，深層学習として有意な結果が得られるロ グ数として実験的に得たものであり，網羅的に検証されたものではない．4 の 2 回以上と いう数字は，各生徒の知識獲得の推移を観察する上で最低限必要なログ数である．結果的 に，3,410 人の，56 の知識タグが紐づく 2,635 の問題に対する，129,317 の回答ログが分析 対象である．

4.2 Bridge to Algebra 2006-2007
本データセットが利用された「KDDCup」について概説した後，データセット自体につ いて説明する．その際，ASSISTments 同様，本研究に適用する際に問題となるデータの 性質に言及した上で，その問題点を解消するためのデータの抽出方法を述べる．

46

第 4 章 データセット

47

(a) 「ASSISTments 2009-2010」

(b) 「Bridge to Algebra 2006-2007」

図 4.1: 問題ごとの回答数の分布
4.2.1 KDDCup
「KDDCup（Knowledge Discovery and Data Mining Cup）3」は、100 以上の国にまた がり、10 万人を超える会員を持つコンピューターサイエンス分野の学会である「ACM(the Association for Computing Machinery)」の分科会である「SIGKDD（Special Interest Group on Knowledge Discovery and Data Mining）」が毎年開催する競技会であり、この 分野で最も古く権威のある競技会の一つである．

4.2.2 対象データセット
本研究では，2010 年に開催された KDDCup の内の一つの，教育分野の競技会である「Educational Data Mining Challenge」で使用された「，Bridge to Algebra 2006-2007」[Stamper et al., 2010] というデータセットを用いる．これは，オンライン教育サービスの「Carnegie Learning4」が提供するオンライン学習支援システム「Cognitive Tutor」における，2006 年から 2007 年の間の，数学の問題に対する生徒の問題回答ログである．Cognitive Tutor は ASSISTments と同じく ITS だが，やや性質の異なるものになっている．ASSISTments は，生徒が毎日の宿題を解く過程をサポートするような，比較的単純な設計となっている 一方で，Cognitive Tutor は，より生徒が個別の知識を獲得する過程を緻密にサポートす る設計となっている．具体的には，各問題 (problem) が，複数のステップ (step) に分解さ れており，ステップ 1 つ 1 つに知識タグ (knowledge component) が紐付いている．この データセットには，1,146 人の生徒の，494 の知識タグが紐づく 19,186 の問題と 19,766 の

3http://www.kdd.org/kdd-cup 4https://www.carnegielearning.com/

47

第 4 章 データセット ステップに対する，3,679,199 の回答ログが含まれている．

48

4.2.3 データの抽出
本データセットも，「ASSISTments 2009-2010」同様に，本研究に適用する上で 3 つの問 題を抱えている．以下に各問題と対策を述べる．
まず，問題 (problem) やステップ (step) という粒度が存在する本データセットにおいて， 何を一回の問題回答と見なしてデータを作成するかが問題となる．Cognitive Tutor では， 一つの問題内に複数のステップが用意されており，各ステップについて逐次回答して正答 できるまで取り組み，正答できた場合に次のステップに進むようになっている．そのため， 生徒の知識獲得の推移を観察する上では，一つ一つのステップに注目することが適切だと いえる．よって，本データセットでは，問題内のステップに対する回答を一回の問題回答 と見なし，データを抽出する．
次に，既存の知識タグ (knowledge concept) についても，存在はするものの，名前が割 り当てられていないものが存在し，これらは抽出された知識タグと比較することが不可能 なため，除外する必要がある．
また，本データセットは，全体で見ると十分大規模であるといえるが，個別の問題に関 して言えば，ほとんど回答されていない問題が，全体に対して大きな割合を占めている (図 4.1b を参照)．十分なログ数を保有しない問題は，大規模データから深層学習によって知 識分類を学習するという本研究の目的を満たさないため，ログ数について一定の閾値を設 けてデータセットを切り分けることにより，適切なデータを抽出する必要がある．
以上より，本研究では，元のデータセットから，以下の方法で分析対象とするデータを 抽出している．
1. 問題 (problem) とステップ (step) の組み合わせを一回の問題回答とみなす，
2. 名前が割り当てられている知識タグを持つ問題の回答ログのみを抽出する．
3. 2 のうち，最低 200 回以上回答されている問題の回答ログのみを抽出する．
4. 3 に含まれる問題を，最低 2 回以上回答している生徒に関するログを抽出する．
なお，3 の 200 回以上という具体的な数字は，深層学習として有意な結果が得られるロ グ数として実験的に得たものであり，網羅的に検証されたものではない．4 の 2 回以上と いう数字は，各生徒の知識獲得の推移を観察する上で最低限必要なログ数である．結果的

48

第 4 章 データセット

49

に，1,124 人の，115 の知識タグが紐づく 618 のステップに対する，227,612 の回答ログが 分析対象である．

4.3 データセットの概観
以上の条件から抽出された，本実験に用いるデータセットの統計量を表 4.1 に示す． 表 4.1: 各データセットの統計量

データセット名 ASSISTments 2009-2010 Bridge to Algebra 2006-2007

生徒数 3,410 1,124

問題数 2,635 618

知識タグ数 56
115

ログ数 129,317 227,612

なお，「Bridge to Algebra 2006-2007」は「ASSISTments 2009-2010」に比べて，問題数 に対する知識タグの数の割合が大きく異なっているが，これは「Bridge to Algebra 20062007」では，問題がさらにステップに分割されており，知識タグの粒度がより細かくなっ ているためである．
以上，データセットについて述べた．次章では，実験について述べる．

49

第 5 章 実験
本章では，実験について述べる． まず，実験設定について述べ，その後，実験結果について述べる．
5.1 実験設定
本研究の実験は，大きく以下の３つのブロックに分けられる． 1. 知識分類学習モデルによる知識分類の学習 2. 学習された知識分類の知識獲得予測の性能に関する検証 3. 学習された知識分類の性質に関する比較分析 以下では，順に，実験設定について述べる．
5.1.1 知識分類学習モデルによる知識分類の学習
生徒の問題回答ログに対し，図 3.1 に表される知識分類学習モデルを適用し，回答正誤の 予測性において最適な知識分類表現を抽出する．知識タグ空間の次元数は，既存の知識タ グの次元数と統一し，「ASSISTments 2009-2010」では 56，「Bridge to Algebra 2006-2007」 では 193 とした．実際のモデルにおいては，正答ベクトルと誤答ベクトルを分けてユニッ トを作るため，それぞれ２倍のユニット数で表現されている．
RNN の部分には GRNN を用いる．ハイパーパラメータについては，学習率の初期値を 200，モーメントを 0.98，1 エポックごとに，減衰率 0.99 として学習率を最小学習率 10 ま で減衰させる．また，勾配のノルムの最大値を 0.00001 として [Pascanu et al., 2013] に 従い勾配に制約を設けた．dropout は [Piech et al., 2015] と同様に yt の方向にのみかけ， dropout 率は 0.5 とした．隠れ層のユニット数は 400 として，各重み行列の初期化は [Glorot and Bengio, 2010] にしたがった．時系列方向の誤差逆伝搬は最長で 200 まで伝搬するよ うに制約を設けた．
50

第 5 章 実験

51

これらのハイパーパラメータは実験的に高い予測性能を発揮したため設定しており，網 羅的に探索したわけではない．通常，深層学習の手法はハイパーパラメータの数が非常に 大きく，また，計算コストが大きいため大規模な探索は行えない．Grid Search や Random Search [Bergstra and Bengio, 2012] といった探索手法が提案されているが，専門家が手 で調整した方が優れていることが報告されている [Larochelle et al., 2007, Bergstra and Bengio, 2012]．
最適化手法は，式 3.7 で表される問題回答予測に関する誤差関数 Lp と，式 2.39 で表さ れる問題空間と知識タグ空間の最高性誤差を表す誤差関数 Lr の和である L(式 3.8) を目的 関数として最小化するものである．学習時は [Piech et al., 2015] と同様にミニバッチごと に確率的勾配降下法で目的関数を最小化する．評価指標は AUC スコアを採用する．
一般的な機械学習では，モデルの汎化性能を高めるために訓練データと検証データを分 けるが，２つのデータセットいずれにおいても，全ユーザの問題回答ログを訓練データと して，モデルを学習させた．この設計と知識分類の汎用性に関しては第６章で詳述する．
実装には Theano を用いた [Bergstra et al., 2010, Bastien et al., 2012]．Theano は多次 元行列を含む数学的表現の定義や計算，最適化を効率的に行える Python のライブラリで， 深層学習の研究ではよく利用される．

5.1.2 学習された知識分類の知識獲得予測の性能に関する検証
5.1.1 で得られた問題空間から知識タグ空間への写像行列 P を知識分類表現と見なして DKT を行い，既存の知識分類を用いた場合との精度の比較を行う．また，本手法で抽出 される知識分類と既存の知識分類の差分を明確にするため，以下の方法で作成された知識 分類を用いた場合とも比較を行う．
• DKT を用いず，一般的な事前学習の Autoencoder によって作成された知識分類
• DKT を用いるが，再構成誤差を目的関数に導入しない，一般的な埋め込み (Embedding) によって作成された知識分類
まず，各手法によって得られた写像行列を，以下の条件に基づいて離散化し，DKT におい て最も高い精度を発揮したものをその手法による精度の上界として採用し，「ASSISTments 2009-2010」では Y = 0.70，「Bridge to Algebra 2006-2007」では Y = 0.85 とした．
1. 各問題の写像ベクトルにおいて，最も値の高いタグを１とする
2. 各問題の写像ベクトルにおいて，値が閾値 Y 以上のタグを１とする
51

第 5 章 実験

52

3. 各問題の写像ベクトルにおいて，1 でない要素を 0 にする
RNN の部分には GRNN を用いる．ハイパーパラメータについては，学習率の初期値を 200，モーメントを 0.98，1 エポックごとに，減衰率 0.95 として学習率を最小学習率 10 ま で減衰させる．また，勾配のノルムの最大値を 0.00001 として [Pascanu et al., 2013] に 従い勾配に制約を設けた．dropout は [Piech et al., 2015] と同様に yt の方向にのみかけ， dropout 率は 0.5 とした．隠れ層のユニット数は 400 として，各重み行列の初期化は [Glorot and Bengio, 2010] にしたがった．時系列方向の誤差逆伝搬は最長で 200 まで伝搬するよ うに制約を設けた．
最適化手法は，一般的な DKT と同じく，式 3.7 で表される回答正誤予測に関する誤差 関数 Lp を目的関数として最小化するものである．学習時は [Piech et al., 2015] と同様に ミニバッチごとに確率的勾配降下法で目的関数を最小化する．評価指標は AUC スコアを 採用する．
２つのデータセットいずれにおいても，訓練：検証：テスト = 8：1：1 となるように ユーザを分け，訓練ユーザのデータでモデルを構築し，検証ユーザのデータでハイパーパ ラメータを調整し，検証ユーザのデータで精度が最も高かったモデルをテストユーザの データに適用し当該モデルの最終的な精度とした．
実装には Theano を用いた．

5.1.3 学習された知識分類の性質に関する比較分析
5.1.1 で抽出された知識分類を既存の知識分類を比較分析することで，その性質を検証 する．
まず，各知識タグが回答ログに出現する頻度の分布や，紐づく問題数の分布に着目し， 知識獲得予測の精度を向上させる要因を，データ構造の側面から分析する．また，既存タ グから成る知識間影響ネットワークに抽出タグを配置して可視化することで，既存タグが 抽出タグによってどのように再配置され，どのようなネットワーク構造となったのかを分 析する．
以上，実験設定について述べた．

5.2 実験結果
実験結果について述べる．まず，各手法によって作成された知識分類についての知識獲 得予測における予測性能を比較し，いずれのデータセットにおいても，提案手法によって
52

第 5 章 実験

53

学習された知識分類表現を利用することで，最も良い精度で予測が可能になっていること を定量的に確認する．
さらに，学習された知識分類を，既存の知識分類と比較することにより，その性質を定 性的に分析する．

5.2.1 各知識分類の知識獲得予測における予測性能
表 5.1: 各知識分類の知識獲得予測における予測性能

データセット

AUC 既存タグ (marginal) 事前学習タグ

ASSISTments 2009-2010 Bridge to Algebra 2006-2007

0.72 (0.61) 0.81 (0.72)

0.67 0.79

深層学習タグ Embedding AutoEncoder
0.69 0.74 0.81 0.82

ベースラインとなる既存の知識タグ (既存タグ) と，知識獲得予測の文脈を考慮しない， 一般的な事前学習の Autoencoder によって作成された知識タグ（事前学習タグ），知識獲 得予測の文脈を考慮し，提案手法の知識分類学習モデルによって作成された知識タグ（深 層学習タグ）をそれぞれ Deep Knowledge Tracing に適用した結果を表 2 に示す．深層学 習の「Embedding」と「Autoencoder」はそれぞれ誤差関数に再構成誤差を導入しない場 合と導入した場合である．marginal は各問題についてそれぞれ正解の周辺確率を予測結果 とするものである．[Piech et al., 2015] にも記載されていたため，本稿でも同様にベース ラインの参考として記載した．また，値が大きい箇所は太字で記載した．
いずれのデータセットにおいても，提案手法である「深層学習タグ (Autoencoder)」が， 最も高い AUC を記録した．この結果より，提案手法によって作成された知識分類が，既 存の知識分類よりも知識獲得の予測性において優れていることが示された．以下，このタ グを「抽出タグ」とする．

5.2.2 抽出タグと既存タグの関係の概観
抽出タグと既存タグの関係を概観する．まず，既存タグをノードとする知識間影響ネッ トワーク（以下，既存タグネットワーク）を，既存の DKT で用いられた手法に基づいて 作成しておく (図 5.1)．次に，抽出タグの紐づく問題を媒介として作成した抽出タグと既 存タグの共起行列に対し，TF-IDF 法を適用して各抽出タグの特徴を強調し，抽出タグと
53

第 5 章 実験

54

既存タグの関係性を表す行列 (以下，タグ関係行列) を作成し，この行列を元に抽出タグの ノードを既存タグネットワークに追加することで，両タグの関係性を表す「タグ関係ネッ トワーク」を作成した (図 5.2)．

(a) 「ASSISTments 2009-2010」

(b) 「Bridge to Algebra 2006-2007」

図 5.1: 既存タグネットワークの構造

(a) 「ASSISTments 2009-2010」

(b) 「Bridge to Algebra 2006-2007」

図 5.2: タグ関係ネットワークの構造

54

第 5 章 実験

55

ここで，図 5.1 の既存タグネットワークでは，ノードのサイズは，各既存タグの回答ロ グにおける出現回数に比例して設定し，ノードの色は，回答ログにおける平均回答順序が 早いものほど青く，遅いものほど赤く色を設定し，各既存タグへの影響度が大きい上位３ つの既存タグからエッジを引いている．また，図 5.2 において追加される抽出タグは緑色 のノードで表現し，各抽出タグにとって関係性の強い上位３つの既存タグに対して緑色の エッジを引いている．
以上の処理の流れを図 5.3 にまとめた．

図 5.3: ネットワーク作成の流れ
なお，タグ関係ネットワークにおいて接続している抽出タグと既存タグの組み合わせは 付録にまとめた．括弧内の数字は TF-IDF 値であり，この値が高いほど各抽出タグにとっ て特徴的な既存タグであることを表す．
5.2.3 抽出タグと既存タグの比較分析
抽出タグを既存タグと比較することにより，抽出タグの性質を定性的に確認する． まず，知識獲得予測の精度を向上させる要因を，データの構造から分析した．抽出タグ と既存タグの各知識タグが回答ログに出現する頻度の分布の比較と，各分布の標準偏差を 図 5.4 に表す．図より，既存タグはタグごとの出現回数の分散が大きい一方で，抽出タグ は分散が小さく，特定の値の周辺に集中していることがわかる．この分布の違いと予測精 度の関係性についても，次章で考察する． 次に，図 5.2 のネットワークの局所的な特性に着目し，抽出タグと既存タグの性質が観 測できる部分を示す．まず，既存タグに注目し，各既存タグに抽出タグがどのように紐付 いているかを観察すると，出現回数の多い既存タグ (大きいノード) は多くの抽出タグ (緑
55

第 5 章 実験

56

(a) 「ASSISTments 2009-2010」

(b) 「Bridge to Algebra 2006-2007」

図 5.4: 各タグの出現回数の分布
のノード) が紐付いている (図 5.5) 一方，出現回数の少ない既存タグ (小さいノード) は特 定の抽出タグのみ紐付き，多くは紐付いていない (図 5.6) ことがわかる．

(a) 「ASSISTments 2009-2010」

(b) 「Bridge to Algebra 2006-2007」

図 5.5: 多くの抽出タグが紐づく既存タグ

(a) 「ASSISTments 2009-2010」

(b) 「Bridge to Algebra 2006-2007」

図 5.6: 少数の抽出タグのみ紐づく既存タグ

56

第 5 章 実験

57

次に，抽出タグに注目し，各抽出タグがどのような既存タグに紐付いているかを観察す ると，内容的な関係性の強い複数の既存タグに紐付いている抽出タグが存在する (図 5.7) ことがわかる．このような既存タグと抽出タグの関係性から，どのような性質のタグが抽 出されているといえるかは，次章で考察する．

(a) 「ASSISTments 2009-2010」

(b) 「Bridge to Algebra 2006-2007」

図 5.7: 内容的関係性の強い既存タグに紐づく抽出タグ

以上，実験について述べた．次章では，考察について述べる．

57

第 6 章 考察
本章では，実験結果を踏まえた考察を述べる． まず，知識獲得予測の性能の比較実験の結果から，本研究で用いた知識分類学習モデル の有効性について考察する．次に，同モデルにより抽出された知識タグと既存の知識タグ を比較し，それぞれの性質や，知識獲得予測に与える影響について考察する．また，本研 究や関連研究が対象としたデータの範囲から，本研究の手法の教育学における他のデータ への適用可能性について考察する．以上の考察を受けて，本研究の成果を実際の教育現場 に適用し，活用する方法について考察する． 最後に，今後の展望として，本研究で用いた手法の教育学での適用の拡大について述べ た後，最後に，教育学以外の分野への応用可能性について述べる．
6.1 知識分類学習モデルと抽出タグの解釈
まず，本手法で用いた知識分類学習モデルと，それによって学習された知識タグについ て解釈を行う．
6.1.1 知識分類学習モデルの有効性
知識獲得予測の性能の比較実験の結果から，本研究で用いた知識分類学習モデルの有効 性について考察する．
まず，既存の知識タグを利用した場合と，各手法によって抽出したタグを利用した場合 の，知識獲得予測の精度に関して考察する．
実験結果より，知識獲得予測の文脈を考慮せず，事前学習の AutoEncoder のみから作 成した知識分類を用いた「事前学習タグ」は，既存タグを用いた場合よりも精度が悪かっ た一方で，知識分類学習モデルにより，知識獲得予測を行う過程で最適化した「深層学習 タグ」は，既存タグよりも精度の良いものがあった．この状況を直接的に解釈すれば，知 識分類は，データ構造のみに着目する教師なし学習では，知識獲得予測に有効な表現とし て学習できないが，知識獲得予測という目的に応じた環境情報を制約に加えて学習するこ
58

第 6 章 考察

59

とで，その環境と矛盾しない範囲で最適化が行われたと考えられる．また，より教育学的 な文脈を踏まえて解釈を試みれば，問題と知識分類はそれ自体で自明な関係ではなく，問 題を回答する生徒の正誤や知識獲得の推移という状態の観測を通して定義されることで， 適切な分類が可能になると見なすことも可能である．
また，知識獲得予測の文脈を考慮した，知識分類学習モデルを用いた手法においても， 単純な低次元空間への埋め込み (Embedding) では精度が向上しないものの，問題空間と タグ空間の再構成誤差を導入することにより，精度が向上した．これは，データ量の不足 に対する正則化項の導入という，ニューラルネットワークの文脈における，データの量的 側面と，問題の回答正誤と知識タグの理解状態は相互に変換できるはずだという，教育学 の文脈における，データの質的側面と，双方の性質を活かす最適化の要素として，再構成 誤差が効果を発揮したと考えられる．

6.1.2 各知識分類の性質と知識獲得予測に与える影響
次に，既存タグと抽出タグのそれぞれの性質を考察し，それがどのように知識獲得予測 に影響をあたえるのかを考察する．
まず，図??，??に表される，既存タグと抽出タグの問題回答ログにおける出現回数の分 布から，既存タグは分散が大きい一方で，抽出タグは分散が小さく，特定の値の周辺に集 中していることがわかった．既存タグは，人間の可読性や直感的なわかりやすさを重視し て作られており，その知識を問う問題の出現回数は作成時の評価軸に含まれていない．そ のため，基礎的・入門的な問題に関しては多くの生徒に回答される一方，専門的であった り難易度の高い問題に関しては，回答される回数が必然的に少なくなるため，タグ間で出 現回数に差が出る．しかし，DKT のモデルに入力される際には，どの知識タグも均等に １つのユニットで表現されるため，実際の知識獲得過程において各タグが持つ情報量の偏 りを十分に表現できない可能性が高い．一方，抽出タグは，知識獲得予測を最適化する過 程で学習されているため，DKT のモデル構造上，各ユニットが均等に情報量を保つこと で，特定のタグに関する情報量が失われることを防いでいると考えられる．
この性質は，TF-IDF 法を用いた各抽出タグの特徴分析とそれに基づいたネットワーク 図にも現れている．図??–??に表されるように，元々出現回数が少ない専門的な既存タグ (小さいノード) は，少数の抽出タグのみで表現されているが，逆に元々出現回数の多い基 礎的な既存タグ (大きいノード) は，複数の抽出タグにまたがって表現されるなど，より効 率的に情報を保持できるタグ構造が抽出されていることがわかる．

59

第 6 章 考察

60

さらに，こうした情報量の均等な分配構造は，内容と全く無関係に生成されるものでは ないこともわかる．図??，??に表されるように，類似した内容の範囲内で情報量の分配を 行っているタグがいくつか見受けられる．こうしたタグは，情報量を適切に保ちつつ関連 知識一般をカバーするようなタグである可能性が高く，人間が知識を獲得していく過程を 考察する上で示唆に富んでいる．

6.2 本手法の汎用性と実用性
次に，本手法の汎用性と実用性について考察する．
6.2.1 学習された知識分類の汎用性
知識分類学習モデルによって学習された知識分類の汎用性について述べる．一般的な機 械学習では，モデルの汎化性能を高めるためにデータを訓練データと検証データを分ける が，本研究では，全ユーザの問題回答ログを全て訓練データとして，モデルを学習させた． このようなデータの利用の仕方は，訓練データの分布において最適な結果が得られる一 方，未知のデータに対する汎用性が担保されないため，一般的ではない．しかし，本研究 においては，知識獲得予測を最適化する知識分類を抽出することを目的とし，その手段と して，これまでの生徒の知識獲得の過程から，帰納的に最適な知識分類を作成するという 形を取っており，これはすなわち，将来の生徒の知識獲得過程は，過去の生徒の知識獲得 の過程と同じ分布に従うという前提に立っている．また，オンライン教育サービスにおい ては，日々新たな学習回答ログが蓄積されるため，その都度知識分類を更新し，より質の 高い分類を作成できるという環境が整っている．以上より，本研究の問題設定においては， これまでの知識獲得過程を全てまとめて訓練データとしても，学習された知識分類の汎用 性は損なわれず，また，未知のデータが出現した際にも即座にそれを反映し，より適切な 知識分類を作成できると判断したため，知識分類の学習の時点では全データを訓練データ とした．なお，学習された知識分類の性能を検証する DKT の部分においては，既存タグ との精度比較の公平性を期すために，データを訓練・検証・テストに分割している．
一方で，現在観測されているデータと将来観測されているデータの分布が大きく異る可 能性がある場合や，未知のデータが出現した場合に即座にそれを分類に反映させることが 困難な場合には，このようなデータの分割方法は相応しくない．そのような場合には，全 データを訓練・検証・テストに分割することで，学習された分類の汎化性能を保証する必

60

第 6 章 考察

61

要があり，環境や問題設定に応じたデータの扱いや，環境に左右されないモデルの設計が 今後の課題といえる．

6.2.2 本手法の他データへの適用可能性
本研究の分析手法の，他の科目やオンライン教育サービスへの適用可能性について考察 する．
本研究の手法は，データセット作成という事前の処理と，1) 知識分類学習モデルによる 知識分類の学習，2) 学習された知識分類の知識獲得予測性能に関する検証，3) 学習され た知識分類の性質に関する比較分析という３つの分析から構成されていた．
データセット作成については，オンライン教育サービスから収集される問題回答ログ データは，サービスや科目によらず大規模であると考えられる．また，実験の「2) 学習さ れた知識分類の知識獲得予測性能に関する検証」，および「3) 学習された知識分類の性質 に関する比較分析」は，知識分類学習モデルによって，適切な知識分類を学習できるかに 依存する．よって，本手法の他科目や他サービスへの適用可能性は，「1) 知識分類学習モデ ルによる知識分類の学習」に依存すると考えられる．知識分類学習モデルは DKT を拡張 したモデル構造において学習されるため，DKT 自体の他科目や他サービスへの適用可能 性によって，本手法の適用可能性も検証されると考えられる．
そこで，DKT の他科目や他サービスにおける予測性能を考察する． [Piech et al., 2015] では，本研究同様，数学に関するデータセットにおいてのみ，DKT の有効性が検証されていた．那須野ら [参考文献引用] はリクルートが提供するオンライン 教育サービス「学習サプリ」のデータを使って，算数や数学に関するデータセットと地理 や歴史に関するデータセットに DKT を適用した場合，Bayesian Knowledge Tracing から の精度向上という点では大きな差はないことを確認しており，DKT の適用可能性は科目 に依らない可能性がある． また，[Piech et al., 2015] を始めとする既存研究では，モデルへの入力次元には問題に 割り当てられたタグが利用されており，DKT の有効性はタグを用いた場合のみ，検証さ れていた．本研究の実験では，抽出されたタグを比較検証するために，既存のタグが存在 するデータセットを用いたものの，問題回答のみから知識分類を学習できることは，実験 結果から示されており，このことから，既存のタグが存在しないような他科目や他サービ スのデータに対しても，生徒の知識獲得を予測することが可能であることを示している． 一方で，これまで検証されているのは，特定の科目に関する問題回答ログであり，総合

61

第 6 章 考察

62

的な知識レベルを問うような，複数の科目が含まれている問題回答ログへの適用可能性は 示されていない．
また，利用できるデータセットは，生徒が該当のオンライン教育サービスで学習する過 程で，段階的に知識を獲得していく前提のデータのみであり，オンライン教育サービス外 での学習や，生徒ごとの能力差，事前知識などの情報に関しては，DKT が扱うことは難 しい可能性がある．
以上のような考察を踏まえると，本手法は，DKT が分析可能な他サービスや他科目の データに加え，DKT による分析が困難な，事前の知識分類が存在しないデータに対して も適用できるという側面がある一方，複数科目のデータや生徒に関する事前情報など，現 実に即した複雑な情報が多く含まれたデータに対しては，適用可能性が限定的である可能 性もあり，検証が必要である．

6.2.3 教育現場への適用
ここまでの考察を踏まえ，本研究の成果を，実際の教育現場に適用し，活用する方法に ついて考察する．
そもそも知識獲得の予測は，オンライン教育サービスにおいては，生徒の回答正誤の情 報を元に，問題を正答するのに必要な知識を生徒が既に獲得しているかを推定することで， 不足している知識を補ったり，既に獲得している知識を除外したりと，適切な順序で教材 推薦を行うことが主な目的であった．
実験結果から，本手法によって抽出された知識分類を利用することによって知識獲得の 予測精度が向上することが確認された．知識獲得の予測精度が向上するということは，各 生徒の知識状態をより的確に把握して，教材推薦の精度を向上させることを意味する．よっ て，現在オンライン教育サービス上で提供されている問題に対して，既存の可読性重視の 知識タグに加え，本研究で抽出された知識タグを紐付けておくことで，教材推薦の精度を 向上させ，生徒個人への教材の最適化が進み，生徒の学習効率をより高めることができる． この知識タグの粒度は，本研究では既存タグとの比較のために固定としていたが，自由に 設定することが可能なため，サービスごとに適切な粒度を設定することが可能である．ま た，本手法は人間による事前の知識分類を必要としないため，人間によって問題が事前に 分類されていないようなサービスに対しても，自動で最適な知識分類を作成し，知識獲得 予測を行うことも可能である．
このように，知識獲得の予測性能を最適化する知識分類を抽出する本手法は，オンライ

62

第 6 章 考察

63

ン教育サービスの教材推薦システムに組み込むことで，より生徒の学習効率を高めること ができる上，適切な知識分類が存在しないサービスにおいても知識獲得予測を可能にする ものである．

6.3 今後の展望
本研究の今後の展望について大きく 3 つの方針を述べる．まず，本手法をさらに改善し， より良質な知識分類を学習できるようなモデル構造の可能性について述べ，次に，教育学 における対象データの拡大について述べ，最後に，教育学以外の分野への本手法の応用に ついて述べる．
6.3.1 知識分類学習モデルの改善
本研究では，知識獲得予測性を最適化する知識分類を抽出するにあたり，まず，知識分 類学習モデルに問題回答ログを入力し，問題から知識分類への写像関数を学習し，その関 数を離散化することで，タグを得ていたが，連続値の行列を人間の手によって離散化する ことにより，情報量の減少が避けられなかった．また，連続表現の知識分類の予測性能と， 離散表現の知識分類の予測性能が線形関係にあるとは限らないため，最適な連続表現を得 てそれを離散化したとしても，最適な離散表現が得られるとは断言できない．
よって，得られた知識分類が最適な離散表現であることを確実に示すには，初めから離 散表現のタグを深層学習によって学習できることがより望ましい．これはすなわち，問題 の集合の背後に離散的な確率分布が存在することを仮定し，その分布を学習することに よって，最適な離散表現のタグを学習することと言える．データが生成された確率分布を 深層学習によって学習するには，一般的な機械学習の識別モデルとは異なる，生成モデルの 研究領域における，Variational Autoencoder(VAE) の技術を用いる必要がある [Kingma et al., 2014]．従来の VAE で学習できるのは連続的な確率分のみとされていたが，近年の 研究により離散分布についても学習することが可能になった [Maddison et al., 2016, Jang et al., 2016]．この手法を用いて知識分類を学習すれば，入力となる問題を生成する潜在 的な離散分布を仮定し，この分布を学習することにより，勾配法による最適化によって直 接最適な離散表現を得ることが可能だと考えられるため，今後の研究課題である．

63

第 6 章 考察

64

6.3.2 教育学における対象データの拡大
次に，教育学における対象データの拡大について述べる．対象データの拡大とは，科目 や難易度の多様化，予測期間の長期化，そして複数科目の統合である．
まず，科目や難易度の多様化について述べる．本研究では．数学の問題回答ログに対し て深層学習を適用し，知識獲得予測に適した知識タグを得た．これまでの DKT の研究成 果から，数学以外の教科に対しても適用できる可能性は高いが，実際にどのような知識タ グが抽出されるかは分析していない．また，今回扱ったデータセットは，小学校から高校 程度の数学に関する問題回答ログであり，より高度で専門的な大学レベルの学問に適用す る場合についても，どのような知識タグが抽出されるかは分析していない．知識獲得の最 適化に関する知見は，学校側からの指導や生徒自身の学習設計に活用されており，多様な 難易度や科目において知識獲得を最適化する知識構造を明らかにすることは，重要である と考えられる．
次に，予測期間の長期化について述べる．本研究で用いたデータセットの対象期間は， １〜２年程度であった．しかし，DKT を用いた生徒の知識獲得の予測は，それまでの生徒 の知識獲得の過程に依存しており，できるだけ長い期間の知識獲得を分析するほうが，よ り高い精度で知識獲得を予測でき，また，より適切な知識タグを抽出できる可能性は高い．
最後に，複数科目の統合について述べる．本研究や既存研究では，特定の科目について 独立に知識獲得を予測し，知識構造を分析している．しかし，実際の生徒の学習の成長過 程は，科目間で完全に独立であるとはいえず，例えば，歴史と地理や，数学と物理などの 科目間では，知識獲得の過程が密接に関係している可能性がある．一人の生徒の，科目を 横断した知識獲得過程に関する研究は，これまで報告されていないが，複数科目を統合し たデータに対して本手法を適用し，科目を横断した知識分類や包括的な学習設計に関する 知見を得ることは，学術的な意義が大きいといえる．

6.3.3 教育学以外の分野への応用
最後に，本手法の教育学以外の分野への応用について述べる． 本手法は，Knowledge Tracing という，教育学の，知識獲得予測タスクにおける手法だ が，より手法を一般化することで，教育学以外の分野にも応用できる可能性を秘めている． 本手法は，生徒の時系列問題回答ログから，回答を重ねるごとに遷移していく知識状態 をモデリングし，知識獲得の過程を適切に表現する知識タグを抽出するというものだが， これをより一般化して捉えると，人間の，何らかのコンテンツ集合に対する時系列行動ロ
64

第 6 章 考察

65

グから，行動を重ねるごとに遷移していく人間の何らかの状態をモデリングし，行動の遷 移を適切に説明する分類表現を抽出しているといえる．教育学の知識獲得の分野において は，このコンテンツ集合に対する行動が生徒の問題回答であり，問題回答により遷移する 生徒の知識状態をモデリングしているが，これと同様のことは，教育学に限らず行える可 能性がある．例えば，消費者が商品を購買するログを分析することで，消費者の嗜好が遷 移する過程をモデリングし，従来の商品分類と異なる，消費者の嗜好の遷移を反映した分 類を抽出することが可能になり，消費者の購買予測がより適切になる可能性がある．教育 学では，問題回答の正誤と知識の間の特殊な関係性をモデル設計に反映しているように， 手法を適用する領域によって調整は必要であるが，コンテンツに対する行動の時系列性を 反映した分類を抽出できる可能性は高く，様々な領域で，学術的にも，実用的にも，価値 の高い知見を得られると考えられる．
以上，考察について述べた．次章では，結論を述べる．

65

第 7 章 結論
本論文では，既存の知識獲得予測における問題として，人間が作成した知識分類を利用 していることを指摘し，深層学習を適用することによって，知識獲得の予測において最適 化された知識分類を抽出できることを検証した
また，抽出された知識分類を定性的に分析することにより，分野の内容的な関係性と， 分類ごとの情報量を最適に分配することが，知識獲得の予測性の向上に寄与することがわ かった．
本研究の成果を実際の教育現場で活用する例の一つとして，教材推薦システムへの適用 を論じた．
また，本研究の分析手法の他の科目やオンライン教育サービスへの適用可能性について 議論し，科目や既存の知識分類の有無によらずに最適な知識分類を学習して知識獲得を予 測できる可能性があること，および，複数科目を統合した知識獲得や，大学水準の知識獲 得に関する分析については，検証実験を行う必要が有ることを述べた．
さらに，本研究の拡張として，より良質な知識分類を学習するために活用できる最新の 深層学習技術や，適用対象データの拡張について述べ，また，より一般性を高めた議論と して，教育学以外の領域においても本手法が適用できる可能性に触れ，人間行動に関する 多様な知見を発見できる可能性を論じた．
本研究は．教育と情報技術の融合の進展やオンライン教育サービスの普及，教育分野に おける大規模分析の活発化や深層学習の躍進など，ここ数年の多様な領域の進展によって 初めて可能になったものである．本研究が，既存の学問体系の再構築，そして人間の学習 や知識の解明につながると信じている．
66

参考文献
[Abelson, 2008] Abelson, H. (2008). The creation of opencourseware at mit. Journal of Science Education and Technology, 17(2):164–174.
[Aleven et al., 2015] Aleven, V., Sewall, J., Popescu, O., Xhakaj, F., Chand, D., Baker, R., Wang, Y., Siemens, G., Ros´e, C., and Gasevic, D. (2015). The beginning of a beautiful friendship? intelligent tutoring systems and moocs. In International Conference on Artiﬁcial Intelligence in Education, pages 525–528. Springer.
[Bahdanau et al., 2015] Bahdanau, D., Chorowski, J., Serdyuk, D., Brakel, P., and Bengio, Y. (2015). End-to-end attention-based large vocabulary speech recognition. arXiv preprint arXiv:1508.04395.
[Bastien et al., 2012] Bastien, F., Lamblin, P., Pascanu, R., Bergstra, J., Goodfellow, I. J., Bergeron, A., Bouchard, N., and Bengio, Y. (2012). Theano: new features and speed improvements. Deep Learning and Unsupervised Feature Learning NIPS 2012 Workshop.
[Bellman and Corporation, 1957] Bellman, R. and Corporation, R. (1957). Dynamic Programming. Rand Corporation research study. Princeton University Press.
[Bengio et al., 1994] Bengio, Y., Simard, P., and Frasconi, P. (1994). Learning long-term dependencies with gradient descent is diﬃcult. Neural Networks, IEEE Transactions on, 5(2):157–166.
[Bergstra and Bengio, 2012] Bergstra, J. and Bengio, Y. (2012). Random search for hyper-parameter optimization. The Journal of Machine Learning Research, 13(1):281– 305.
[Bergstra et al., 2010] Bergstra, J., Breuleux, O., Bastien, F., Lamblin, P., Pascanu, R., Desjardins, G., Turian, J., Warde-Farley, D., and Bengio, Y. (2010). Theano: a
67

参考文献

68

CPU and GPU math expression compiler. In Proceedings of the Python for Scientiﬁc Computing Conference (SciPy). Oral Presentation.
[Biswas et al., 2015] Biswas, S., Chadda, E., and Ahmad, F. (2015). Sentiment analysis with gated recurrent units. Advances in Computer Science and Information Technology.
[Bloom, 1968] Bloom, B. S. (1968). Learning for mastery. instruction and curriculum. regional education laboratory for the carolinas and virginia, topical papers and reprints, number 1. Evaluation comment, 1(2):n2.
[Carbonell, 1970] Carbonell, J. R. (1970). Ai in cai: An artiﬁcial-intelligence approach to computer-assisted instruction. IEEE transactions on man-machine systems, 11(4):190–202.
[Chen et al., 2008] Chen, N.-S., Wei, C.-W., Chen, H.-J., et al. (2008). Mining e-learning domain concept map from academic articles. Computers & Education, 50(3):1009– 1021.
[Cho et al., 2014] Cho, K., Van Merri¨enboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., and Bengio, Y. (2014). Learning phrase representations using rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078.
[Choi et al., 2015] Choi, E., Bahadori, M. T., and Sun, J. (2015). Doctor ai: Predicting clinical events via recurrent neural networks. arXiv preprint arXiv:1511.05942.
[Chung et al., 2014] Chung, J., Gulcehre, C., Cho, K., and Bengio, Y. (2014). Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555.
[Chung et al., 2015] Chung, J., Gulcehre, C., Cho, K., and Bengio, Y. (2015). Gated feedback recurrent neural networks. arXiv preprint arXiv:1502.02367.
[Clevert et al., 2015] Clevert, D.-A., Unterthiner, T., and Hochreiter, S. (2015). Fast and accurate deep network learning by exponential linear units (elus). arXiv preprint arXiv:1511.07289.

68

参考文献

69

[Corbett and Anderson, 1994] Corbett, A. T. and Anderson, J. R. (1994). Knowledge tracing: Modeling the acquisition of procedural knowledge. User modeling and useradapted interaction, 4(4):253–278.
[Dong et al., 2015] Dong, D., Wu, H., He, W., Yu, D., and Wang, H. (2015). Multitask learning for multiple language translation. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing. ACL.
[Erhan et al., 2010] Erhan, D., Bengio, Y., Courville, A., Manzagol, P.-A., Vincent, P., and Bengio, S. (2010). Why does unsupervised pre-training help deep learning? Journal of Machine Learning Research, 11(Feb):625–660.
[FALAKMASIR et al., 2015] FALAKMASIR, M., Yudelson, M., Ritter, S., and Koedinger, K. (2015). Spectral bayesian knowledge tracing. In Proceedings of the 8th International Conference on Educational Data Mining., OC Santos, JG Boticario, C. Romero, M. Pechenizkiy, A. Merceron, P. Mitros, JM Luna, C. Mihaescu, P. Moreno, A. Hershkovitz, S. Ventura, and M. Desmarais, Eds. Madrid, Spain, pages 360–364.
[Friedman, 1997] Friedman, J. H. (1997). On bias, variance, 0/1―loss, and the curseof-dimensionality. Data mining and knowledge discovery, 1(1):55–77.
[Glorot and Bengio, 2010] Glorot, X. and Bengio, Y. (2010). Understanding the diﬃculty of training deep feedforward neural networks. In International conference on artiﬁcial intelligence and statistics, pages 249–256.
[Graves and Schmidhuber, 2009] Graves, A. and Schmidhuber, J. (2009). Oﬄine handwriting recognition with multidimensional recurrent neural networks. In Advances in Neural Information Processing Systems, pages 545–552.
[Hidasi et al., 2015] Hidasi, B., Karatzoglou, A., Baltrunas, L., and Tikk, D. (2015). Session-based recommendations with recurrent neural networks. arXiv preprint arXiv:1511.06939.
[Hinton et al., 2012] Hinton, G., Deng, L., Yu, D., Dahl, G. E., Mohamed, A.-r., Jaitly, N., Senior, A., Vanhoucke, V., Nguyen, P., Sainath, T. N., et al. (2012). Deep neu-
69

参考文献

70

ral networks for acoustic modeling in speech recognition: The shared views of four research groups. Signal Processing Magazine, IEEE, 29(6):82–97.
[Hinton and Salakhutdinov, 2006] Hinton, G. E. and Salakhutdinov, R. R. (2006). Reducing the dimensionality of data with neural networks. Science, 313(5786):504–507.
[Hochreiter, 1998] Hochreiter, S. (1998). The vanishing gradient problem during learning recurrent neural nets and problem solutions. International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems, 6(02):107–116.
[Hochreiter and Schmidhuber, 1997] Hochreiter, S. and Schmidhuber, J. (1997). Long short-term memory. Neural computation, 9(8):1735–1780.
[Jang et al., 2016] Jang, E., Gu, S., and Poole, B. (2016). Categorical reparameterization with gumbel-softmax. arXiv preprint arXiv:1611.01144.
[Karpathy et al., 2015] Karpathy, A., Johnson, J., and Li, F.-F. (2015). Visualizing and understanding recurrent networks. arXiv preprint arXiv:1506.02078.
[Keller, 1968] Keller, F. S. (1968). Good-bye, teacher... Journal of applied behavior analysis, 1(1):79.
[Kingma and Ba, 2014] Kingma, D. and Ba, J. (2014). Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.
[Kingma et al., 2014] Kingma, D. P., Mohamed, S., Rezende, D. J., and Welling, M. (2014). Semi-supervised learning with deep generative models. In Advances in Neural Information Processing Systems, pages 3581–3589.
[Krueger and Memisevic, 2015] Krueger, D. and Memisevic, R. (2015). Regularizing rnns by stabilizing activations. arXiv preprint arXiv:1511.08400.
[Ku et al., 1995] Ku, W., Storer, R. H., and Georgakis, C. (1995). Disturbance detection and isolation by dynamic principal component analysis. Chemometrics and intelligent laboratory systems, 30(1):179–196.
[Kushner and Yin, 2003] Kushner, H. J. and Yin, G. (2003). Stochastic approximation and recursive algorithms and applications, volume 35. Springer Science & Business Media.
70

参考文献

71

[Larochelle et al., 2007] Larochelle, H., Erhan, D., Courville, A., Bergstra, J., and Bengio, Y. (2007). An empirical evaluation of deep architectures on problems with many factors of variation. In Proceedings of the 24th international conference on Machine learning, pages 473–480. ACM.
[Le et al., 2015] Le, Q. V., Jaitly, N., and Hinton, G. E. (2015). A simple way to initialize recurrent networks of rectiﬁed linear units. arXiv preprint arXiv:1504.00941.
[LeCun et al., 1998] LeCun, Y., Bottou, L., Bengio, Y., and Haﬀner, P. (1998). Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324.
[Lipton et al., 2015] Lipton, Z. C., Kale, D. C., Elkan, C., and Wetzell, R. (2015). Learning to diagnose with lstm recurrent neural networks. arXiv preprint arXiv:1511.03677.
[Liyanagunawardena et al., 2013] Liyanagunawardena, T., Williams, S., and Adams, A. (2013). The impact and reach of moocs: A developing countries perspective. eLearning Papers, (33).
[Louradour and Kermorvant, 2014] Louradour, J. and Kermorvant, C. (2014). Curriculum learning for handwritten text line recognition. In Document Analysis Systems (DAS), 2014 11th IAPR International Workshop on, pages 56–60. IEEE.
[MacHardy and Pardos, 2015] MacHardy, Z. and Pardos, Z. A. (2015). Toward the evaluation of educational videos using bayesian knowledge tracing and big data. In Proceedings of the Second (2015) ACM Conference on Learning@ Scale, pages 347– 350. ACM.
[Maddison et al., 2016] Maddison, C. J., Mnih, A., and Teh, Y. W. (2016). The concrete distribution: A continuous relaxation of discrete random variables. arXiv preprint arXiv:1611.00712.
[McAuley et al., 2010] McAuley, A., Stewart, B., Siemens, G., and Cormier, D. (2010). The mooc model for digital practice.
[Midgley, 2014] Midgley, C. (2014). Goals, goal structures, and patterns of adaptive learning. Routledge.

71

参考文献

72

[Mikolov, 2012] Mikolov, T. (2012). Statistical language models based on neural networks. Presentation at Google, Mountain View, 2nd April.
[Minsky and Papert, 1969] Minsky, M. and Papert, S. (1969). Perceptron (expanded edition).
[Nair and Hinton, 2010] Nair, V. and Hinton, G. E. (2010). Rectiﬁed linear units improve restricted boltzmann machines. In Proceedings of the 27th International Conference on Machine Learning (ICML-10), pages 807–814.
[Pappano, 2012] Pappano, L. (2012). The year of the mooc. The New York Times, 2(12):2012.
[Pascanu et al., 2013] Pascanu, R., Mikolov, T., and Bengio, Y. (2013). On the diﬃculty of training recurrent neural networks. arXiv preprint arXiv:1211.5063.
[Pavlik Jr et al., 2009] Pavlik Jr, P. I., Cen, H., and Koedinger, K. R. (2009). Performance factors analysis–a new alternative to knowledge tracing. Online Submission.
[Pearson, 1901] Pearson, K. (1901). Liii. on lines and planes of closest ﬁt to systems of points in space. The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science, 2(11):559–572.
[Pezeshki, 2015] Pezeshki, M. (2015). Sequence modeling using gated recurrent neural networks. arXiv preprint arXiv:1501.00299.
[Piccioli, 2014] Piccioli, V. (2014). E-learning market trends & forecast 2014-2016 report. Athens (GA)-USA.
[Piech et al., 2015] Piech, C., Bassen, J., Huang, J., Ganguli, S., Sahami, M., Guibas, L. J., and Sohl-Dickstein, J. (2015). Deep knowledge tracing. In Advances in Neural Information Processing Systems, pages 505–513.
[Robbins and Monro, 1951] Robbins, H. and Monro, S. (1951). A stochastic approximation method. The annals of mathematical statistics, pages 400–407.
[Rosenblatt, 1958] Rosenblatt, F. (1958). The perceptron: a probabilistic model for information storage and organization in the brain. Psychological review, 65(6):386.

72

参考文献

73

[Rumelhart et al., 1988] Rumelhart, D. E., Hinton, G. E., and Williams, R. J. (1988). Learning representations by back-propagating errors. Cognitive modeling, 5(3):1.
[Sak et al., 2015] Sak, H., Senior, A., Rao, K., and Beaufays, F. (2015). Fast and accurate recurrent neural network acoustic models for speech recognition. arXiv preprint arXiv:1507.06947.
[Sch¨olkopf et al., 1997] Scho¨lkopf, B., Smola, A., and Mu¨ller, K.-R. (1997). Kernel principal component analysis. In International Conference on Artiﬁcial Neural Networks, pages 583–588. Springer.
[Schroﬀ et al., 2015] Schroﬀ, F., Kalenichenko, D., and Philbin, J. (2015). Facenet: A uniﬁed embedding for face recognition and clustering. arXiv preprint arXiv:1503.03832.
[Siemens, 2013] Siemens, G. (2013). Massive open online courses: Innovation in education. Open educational resources: Innovation, research and practice, 5.
[Silver et al., 2016] Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van Den Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M., et al. (2016). Mastering the game of go with deep neural networks and tree search. Nature, 529(7587):484–489.
[Sleeman and Brown, 1982] Sleeman, D. and Brown, J. (1982). Intelligent tutoring systems. Computers and people series. Academic Press.
[Srivastava et al., 2014] Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. (2014). Dropout: A simple way to prevent neural networks from overﬁtting. The Journal of Machine Learning Research, 15(1):1929–1958.
[Stamper et al., 2010] Stamper, J., Niculescu-Mizil, A., Ritter, S., Gordon, G., and Koedinger, K. (2010). Bridge to algebra 2006-2007. development data set from kdd cup 2010 educational data mining challenge. http://pslcdatashop.web.cmu.edu/KDDCup/downloads.jsp.
[Sutskever et al., 2014] Sutskever, I., Vinyals, O., and Le, Q. V. (2014). Sequence to sequence learning with neural networks. In Advances in neural information processing systems, pages 3104–3112.
73

参考文献

74

[Szegedy et al., 2014] Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., and Rabinovich, A. (2014). Going deeper with convolutions. arXiv preprint arXiv:1409.4842.
[Tetko et al., 1995] Tetko, I. V., Livingstone, D. J., and Luik, A. I. (1995). Neural network studies. 1. comparison of overﬁtting and overtraining. Journal of chemical information and computer sciences, 35(5):826–833.
[Tipping and Bishop, 1999] Tipping, M. E. and Bishop, C. M. (1999). Probabilistic principal component analysis. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 61(3):611–622.
[Trucano et al., 2013] Trucano, M., Kendrick, C., and Gashurov, I. (2013). More about moocs and developing countries.
[Upbin, 2012] Upbin, B. (2012). Knewton is building the world s smartest tutor.
[Vincent et al., 2008] Vincent, P., Larochelle, H., Bengio, Y., and Manzagol, P.-A. (2008). Extracting and composing robust features with denoising autoencoders. In Proceedings of the 25th international conference on Machine learning, pages 1096– 1103. ACM.
[Vinyals et al., 2014] Vinyals, O., Toshev, A., Bengio, S., and Erhan, D. (2014). Show and tell: A neural image caption generator. arXiv preprint arXiv:1411.4555.
[Vondrick et al., 2016] Vondrick, C., Pirsiavash, H., and Torralba, A. (2016). Generating videos with scene dynamics. In Advances In Neural Information Processing Systems, pages 613–621.
[Werbos, 1990] Werbos, P. J. (1990). Backpropagation through time: what it does and how to do it. Proceedings of the IEEE, 78(10):1550–1560.
[Williams and Zipser, 1989] Williams, R. J. and Zipser, D. (1989). A learning algorithm for continually running fully recurrent neural networks. Neural computation, 1(2):270– 280.
[Wold et al., 1987] Wold, S., Esbensen, K., and Geladi, P. (1987). Principal component analysis. Chemometrics and intelligent laboratory systems, 2(1-3):37–52.
74

参考文献

75

[Wu et al., 2016] Wu, Y., Schuster, M., Chen, Z., Le, Q. V., Norouzi, M., Macherey, W., Krikun, M., Cao, Y., Gao, Q., Macherey, K., et al. (2016). Google’s neural machine translation system: Bridging the gap between human and machine translation. arXiv preprint arXiv:1609.08144.
[Xiong et al., 2016a] Xiong, W., Droppo, J., Huang, X., Seide, F., Seltzer, M., Stolcke, A., Yu, D., and Zweig, G. (2016a). Achieving human parity in conversational speech recognition. arXiv preprint arXiv:1610.05256.
[Xiong et al., 2016b] Xiong, X., Zhao, S., Van Inwegen, E. G., and Beck, J. E. (2016b). Going deeper with deep knowledge tracing. In Proceedings of the 9th International Conference on Educational Data Mining (EDM 2016), pages 545–550.
[Xu et al., 2015] Xu, K., Ba, J., Kiros, R., Courville, A., Salakhutdinov, R., Zemel, R., and Bengio, Y. (2015). Show, attend and tell: Neural image caption generation with visual attention. arXiv preprint arXiv:1502.03044.
[Yin et al., 2015] Yin, J., Jiang, X., Lu, Z., Shang, L., Li, H., and Li, X. (2015). Neural generative question answering. arXiv preprint arXiv:1512.01337.
[Yuan et al., 2013] Yuan, L., Powell, S., and CETIS, J. (2013). Moocs and open education: Implications for higher education.
[Yudelson et al., 2013] Yudelson, M. V., Koedinger, K. R., and Gordon, G. J. (2013). Individualized bayesian knowledge tracing models. In Artiﬁcial Intelligence in Education, pages 171–180. Springer.
[Zaremba, 2015] Zaremba, W. (2015). An empirical exploration of recurrent network architectures.
[Zeiler, 2012] Zeiler, M. D. (2012). Adadelta: An adaptive learning rate method. arXiv preprint arXiv:1212.5701.
[三宅なほみ et al., 2002] 三宅なほみ, 三宅芳雄, and 白水始 (2002). 学習環境のデザイン 実験 学習科学と認知科学. 認知科学, 9(3):328–337.
[白水始 et al., 2014] 白水始, 三宅なほみ, and 益川弘如 (2014). 学習科学の新展開. 認知 科学, 21(2):254–267.

75

付録

タグ関係ネットワークにおける抽出タグと既存タグの対応関係

表 1: 「ASSISTments 2009-2010」

抽出タグ TF-IDF 値  既存タグ

0

1.076

Addition and Subtraction Integers

0.172

Probability of a Single Event

0.172

Addition and Subtraction Fractions

1

0.172

Addition and Subtraction Integers

0.080

Calculations with Similar Figures

0.154

Addition and Subtraction Fractions

0.136

Probability of a Single Event

2

0.104

Absolute Value

0.050

Ordering Positive Decimals

0.033

Scientiﬁc Notation

0.209

Probability of a Single Event

0.096

Box and Whisker

0.082

Median

3

0.075

Pythagorean Theorem

0.047

Write Linear Equation from Ordered Pairs

0.038

Volume Cylinder

0.254

Addition and Subtraction Fractions

0.181

Probability of a Single Event

4

0.089

Percent Of

0.055

Order of Operations +,-,/,* () positive reals

0.143

Probability of a Single Event

次ページに続く
5

76

付録：タグ関係ネットワークにおける抽出タグと既存タグの対応関係

77

前ページからの続き
抽出タグ TF-IDF 値  既存タグ

0.106

Table

0.094

Mean

0.064

Range

0.198

Addition and Subtraction Fractions

6

0.184

Probability of a Single Event

0.149

Percent Of

0.146

Addition and Subtraction Fractions

0.134

Probability of a Single Event

7

0.115

Multiplication Fractions

0.061

Scatter Plot

0.252

Addition and Subtraction Fractions

8

0.185

Probability of a Single Event

0.141

Percent Of

0.280

Addition and Subtraction Fractions

9

0.135

Probability of a Single Event

0.085

Percent Of

0.138

Absolute Value

10

0.092

Probability of a Single Event

0.091

Conversion of Fraction Decimals Percents

0.185

Probability of a Single Event

11

0.167

Addition and Subtraction Integers

0.135

Equation Solving Two or Fewer Steps

0.188

Addition and Subtraction Fractions

0.162

Probability of a Single Event

12

0.083

Box and Whisker

0.040

Interpreting Coordinate Graphs

0.436

Probability of a Single Event

13

0.215

Percent Of

次ページに続く

77

付録：タグ関係ネットワークにおける抽出タグと既存タグの対応関係

78

前ページからの続き
抽出タグ TF-IDF 値  既存タグ

0.064

Median

0.118

Addition and Subtraction Fractions

14

0.085

Addition and Subtraction Positive Decimals

0.079

Order of Operations All

0.237

Probability of a Single Event

15

0.174

Addition and Subtraction Positive Decimals

0.136

Percent Of

0.184

Pattern Finding

0.125

Subtraction Whole Numbers

16

0.110

Equation Solving Two or Fewer Steps

0.076

Multiplication and Division Integers

0.217

Addition and Subtraction Positive Decimals

17

0.194

Probability of a Single Event

0.136

Conversion of Fraction Decimals Percents

0.237

Probability of a Single Event

18

0.154

Addition and Subtraction Fractions

0.090

Box and Whisker

0.110

Probability of a Single Event

19

0.085

Addition and Subtraction Fractions

0.085

Addition and Subtraction Integers

0.220

Addition and Subtraction Fractions

20

0.105

Probability of a Single Event

0.100

Percent Of

0.230

Probability of a Single Event

0.146

Conversion of Fraction Decimals Percents

21

0.080

Equation Solving Two or Fewer Steps

0.046

Area Irregular Figure

0.045

Square Root

0.188

Probability of a Single Event

22 次ページに続く

78

付録：タグ関係ネットワークにおける抽出タグと既存タグの対応関係

79

前ページからの続き
抽出タグ TF-IDF 値  既存タグ

0.108

Addition and Subtraction Fractions

0.088

Median

0.183

Finding Percents

0.146

Pattern Finding

0.118

Addition and Subtraction Positive Decimals

23

0.040

Eﬀect of Changing Dimensions of a Shape Prportionally

0.011

Perimeter of a Polygon

0.011

Stem and Leaf Plot

0.164

Equation Solving Two or Fewer Steps

0.100

Addition and Subtraction Integers

24

0.091

Table

0.052

Fraction Of

0.046

Mode

25

1.076

Addition and Subtraction Integers

0.198

Probability of a Single Event

0.132

Addition and Subtraction Fractions

26

0.094

Box and Whisker

0.051

Area Trapezoid

0.201

Probability of a Single Event

0.132

Addition and Subtraction Fractions

27

0.092

Box and Whisker

0.064

Divisibility Rules

0.205

Probability of a Single Event

28

0.205

Addition and Subtraction Fractions

0.092

Box and Whisker

0.191

Probability of a Single Event

29

0.096

Percent Of

0.091

Addition and Subtraction Fractions

0.204

Addition and Subtraction Fractions

次ページに続く
30
79

付録：タグ関係ネットワークにおける抽出タグと既存タグの対応関係

80

前ページからの続き
抽出タグ TF-IDF 値  既存タグ

0.133

Probability of a Single Event

0.110

Conversion of Fraction Decimals Percents

0.091

Ordering Fractions

0.931

Write Linear Equation from Graph

31

0.359

Addition and Subtraction Integers

0.236

Probability of a Single Event

0.202

Percent Of

32

0.147

Addition and Subtraction Fractions

0.062

Addition Whole Numbers

0.230

Addition and Subtraction Positive Decimals

33

0.097

Order of Operations All

0.076

Conversion of Fraction Decimals Percents

0.227

Probability of a Single Event

34

0.191

Addition and Subtraction Fractions

0.139

Percent Of

0.173

Probability of a Single Event

35

0.135

Addition and Subtraction Fractions

0.113

Addition and Subtraction Integers

0.189

Probability of a Single Event

0.189

Addition and Subtraction Fractions

36

0.096

Percent Of

0.036

Estimation

0.264

Probability of a Single Event

37

0.244

Addition and Subtraction Fractions

0.193

Percent Of

0.223

Pattern Finding

0.152

Subtraction Whole Numbers

38

0.109

Finding Percents

次ページに続く

80

付録：タグ関係ネットワークにおける抽出タグと既存タグの対応関係

81

前ページからの続き
抽出タグ TF-IDF 値  既存タグ

0.050

Histogram as Table or Graph

0.158

Probability of a Single Event

39

0.130

Addition and Subtraction Fractions

0.108

Percent Of

0.258

Addition and Subtraction Fractions

40

0.167

Probability of a Single Event

0.130

Box and Whisker

0.302

Finding Percents

41

0.189

Addition and Subtraction Positive Decimals

0.099

Conversion of Fraction Decimals Percents

0.121

Addition and Subtraction Positive Decimals

42

0.111

Probability of a Single Event

0.090

Addition and Subtraction Fractions

0.249

Probability of a Single Event

43

0.184

Addition and Subtraction Fractions

0.125

Percent Of

0.247

Addition and Subtraction Fractions

44

0.129

Probability of a Single Event

0.108

Percent Of

0.212

Probability of a Single Event

45

0.173

Addition and Subtraction Fractions

0.100

Box and Whisker

0.204

Probability of a Single Event

46

0.185

Addition and Subtraction Fractions

0.118

Percent Of

0.326

Surface Area Rectangular Prism

0.272

Pattern Finding

47

0.205

Equation Solving More Than Two Steps

次ページに続く

81

付録：タグ関係ネットワークにおける抽出タグと既存タグの対応関係

82

前ページからの続き
抽出タグ TF-IDF 値  既存タグ

0.175

Venn Diagram

0.145

Probability of a Single Event

48

0.129

Equation Solving Two or Fewer Steps

0.099

Addition and Subtraction Fractions

0.145

Addition and Subtraction Positive Decimals

0.081

Equation Solving Two or Fewer Steps

49

0.064

Equivalent Fractions

0.061

Division Fractions

0.034

Volume Sphere

0.111

Probability of a Single Event

0.111

Addition and Subtraction Integers

50

0.090

Probability of Two Distinct Events

0.074

Counting Methods

0.071

Proportion

0.134

Equation Solving Two or Fewer Steps

51

0.121

Addition and Subtraction Positive Decimals

0.115

Probability of a Single Event

0.175

Probability of a Single Event

0.128

Percent Of

52

0.081

Addition and Subtraction Fractions

0.079

Circle Graph

0.054

Unit Rate

0.372

Equation Solving Two or Fewer Steps

0.203

Order of Operations All

53

0.130

Probability of a Single Event

0.073

Least Common Multiple

0.072

Circumference

0.538

Probability of a Single Event

54

次ページに続く

82

付録：タグ関係ネットワークにおける抽出タグと既存タグの対応関係

83

前ページからの続き
抽出タグ TF-IDF 値  既存タグ

0.538

Addition and Subtraction Fractions

以上

表 2: 「Bridge to Algebra 2006-2007」

抽出タグ TF-IDF 値  既存タグ

0.388

Identify common denominator

0

0.344

Rewrite fraction with common denominator

0.290

Identify number of equal divisions on number line from desired denominator

0.707

Identify Fraction using fraction shape

1

0.561

Calculate product of two numbers

0.561

Identify GCF in written question

0.842

Enter smaller inital in diagram – calculated

2

0.774

Draw larger bar – addition/subtraction

0.726

Identify number of desired groups

0.442

Calculate diﬀerence with positive integer

3

0.357

Identify multiplier in written question – number line

0.304

Identify fraction associated with each piece of a square

0.774

Draw larger bar – addition/subtraction

4

0.689

Draw smaller bar – addition/subtraction

0.518

Identify multiplier in equivalence statement

0.383

Identify number as common factor

5

0.293

Identify number of items

0.220

Calculate Part

0.574

Represent multiplier visually

6

0.574

Calculate product – multiply statement

0.355

Label equivalent fraction on number line

0.574

Identify equal parts for multiplier

7

0.440

Identify number as common factor

次ページに続く

83

付録：タグ関係ネットワークにおける抽出タグと既存タグの対応関係

84

前ページからの続き
抽出タグ TF-IDF 値  既存タグ

0.436

Enter subtracted quantity in diagram

1.590

List consecutive multiples of a number

8

0.734

Identify number as common factor

1.436

Identify that a fraction can/cannot be simpliﬁed

9

1.090

Enter items denominator

0.837

Identify larger quantity – multiplication

0.957

Identify whole number of improper fraction symbolically

10

0.734

Identify number as common factor

0.726

Identify fractional part of improper fraction symbolically

0.611

Rewrite fraction with common denominator

11

0.536

Identify LCM - is product

0.407

Compare fractions with like denominators

0.410

Identify width of overlap

12

0.332

Identify number of equal divisions on number line from desired denominator

0.311

Represent second fraction on number line as diﬀerence

1.149

Represent negative integer using number line

13

0.733

Enter added quantity in diagram

0.550

Identify improper fraction from option 2

1.090

Calculate sum – contextual

14

0.861

Count number of shaded parts in square (discontiguous)

0.728

Draw larger bar – multiplication

1.684

Identify number of equal divisions in visual from fraction

15

0.720

Identify number of recipients

0.489

Identify number as common factor

0.957

Rewrite adding positive integer

16

0.842

Represent ﬁrst integer on number line

0.720

Identify number of recipients

2.526

Identify number of equal divisions (vertical bar)

17

次ページに続く

84

付録：タグ関係ネットワークにおける抽出タグと既存タグの対応関係

85

前ページからの続き
抽出タグ TF-IDF 値  既存タグ

0.734

Identify number as common factor

0.957

Identify number of desired groups–construct

18

0.720

Identify number of recipients

0.485

Draw larger bar – multiplication

1.090

Identify number of desired groups

19

1.034

Identify negative integer from number line

0.795

Identify Fraction using fraction shape

0.718

Represent multiplicand visually

20

0.718

Identify fraction associated with each piece of a horizontal bar

0.652

Identify improper fraction from option 1

1.436

Draw smaller bar – multiplication

21

1.263

Identify fraction of desired items

1.263

Identify fraction associated with each piece of a circle

22

3.953

Calculate sum – non contextual

0.734

Identify number as common factor

23

0.689

Identify number of equal groups from fraction

0.485

List factor of large number

0.971

Draw larger bar – multiplication

24

0.749

Identify number of items

0.489

Identify number as common factor

2.179

Identify number of items in each group

25

1.456

List factor of large number

0.821

Identify larger quantity – subtraction

26

0.722

Identify GCF in written question

0.629

Identify number as common factor

1.436

Identify whole number upper bound

27

1.263

Identify number of equal divisions (circle)

1.090

Identify fractional part of improper fraction symbolically

次ページに続く

85

付録：タグ関係ネットワークにおける抽出タグと既存タグの対応関係

86

前ページからの続き
抽出タグ TF-IDF 値  既存タグ

0.774

Identify multiplier in written question – rectangle

28

0.726

Enter subtracted quantity in diagram

0.633

Label equivalent fraction in equivalence statement

2.872

Write improper fraction as mixed number

29

1.833

Compare fractions with like denominators

1.222

Calculate diﬀerence – contextual

30

0.957

Identify GCF

0.842

Calculate diﬀerence – non contextual

1.010

Enter smaller inital in diagram – calculated

31

0.760

Represent ﬁrst fraction on number line

0.550

Rewrite fraction with common denominator

2.526

Calculate Total

32

1.206

Identify LCM - is product

0.383

Identify whole number lower bound

33

0.337

Identify fraction associated with each piece of a vertical bar

0.294

Identify number as common factor

1.305

Identify improper fraction from option 1

34

1.124

Identify number of items

0.916

Identify GCF - one number multiple of other

35

0.687

Identify improper fraction from option 2

0.603

Identify LCM - is product

0.397

Identify number of items

36

0.338

Enter smaller initial in diagram – given

0.323

Compare Options - operation

37

2.161

Identify number of recipients

1.149

Write mixed number as improper fraction

38

0.760

Represent ﬁrst fraction on number line

0.710

Label equivalent fraction on number line

次ページに続く

86

付録：タグ関係ネットワークにおける抽出タグと既存タグの対応関係

87

前ページからの続き
抽出タグ TF-IDF 値  既存タグ

0.409

Identify number of items

39

0.396

Represent second fraction on number line as diﬀerence

0.393

Identify number of recipients

0.454

Identify Fraction using fraction shape

40

0.321

Identify number of items

0.315

Identify number as common factor

1.553

Identify multiplier in equivalence statement

41

0.652

Identify improper fraction from option 1

0.367

Identify number as common factor

0.718

Identify equal parts for multiplicand

42

0.545

Calculate sum – contextual

0.540

Identify number of recipients

43

2.609

Identify improper fraction from option 1

1.263

Calculate Part

44

0.728

Draw larger bar – multiplication

0.687

Compare Options - operation

0.516

Identify multiplier in written question – number line

45

0.489

Identify number as common factor

0.459

Identify number of equal groups from fraction

46

1.468

Identify number as common factor

0.381

Identify number as common factor

47

0.236

List consecutive multiples of a number

0.213

Identify whole number of mixed number symbolically

0.442

Calculate area of overlap

48

0.339

Identify number as common factor

0.335

Compare fractions from contextual problem

0.536

Identify LCM - is product

49

0.484

Enter items numerator

次ページに続く

87

付録：タグ関係ネットワークにおける抽出タグと既存タグの対応関係

88

前ページからの続き
抽出タグ TF-IDF 値  既存タグ

0.362

Identify larger quantity – addition

1.010

Identify GCF in equivalence statement

50

0.550

Compare Options - operation

0.550

Identify improper fraction from option 2

0.281

Represent ﬁrst fraction on number line

51

0.250

Identify number of items

0.240

Identify number of recipients

0.451

Calculate sum – contextual

52

0.409

Identify fraction associated with each piece of a square

0.373

Identify number of recipients

0.447

Label equivalent fraction in equivalence statement

53

0.397

Identify number of items

0.345

Identify number as common factor

1.149

Represent second positive integer on number line as diﬀerence

54

0.827

Draw smaller bar – addition/subtraction

0.587

Identify number as common factor

1.436

Convert improper fraction to whole number

55

0.728

List factor of large number

0.540

Identify number of recipients

0.629

Identify number as common factor

56

0.543

Enter larger inital in diagram – calculated

0.393

Rewrite fraction with common denominator

0.722

Identify number of equal divisions in visual from fraction

57

0.722

Identify GCF in equivalence statement

0.524

Compare fractions with unlike denominators

0.957

Calculate sum with positive integer

58

0.611

Enter added quantity in diagram

0.458

Identify improper fraction from option 2

次ページに続く

88

付録：タグ関係ネットワークにおける抽出タグと既存タグの対応関係

89

前ページからの続き
抽出タグ TF-IDF 値  既存タグ

0.774

Identify multiplier in written question – rectangle

59

0.726

Enter items numerator

0.591

Label equivalent fraction on number line

0.957

Isolate numerator of fractional part of mixed number symbolically

60

0.957

Enter larger initial in diagram – given

0.842

Identify fraction of desired items

1.206

Identify LCM - is product

61

0.734

Identify number as common factor

0.581

Identify number of desired items

62

0.545

Compare fractions from contextual problem

0.540

Identify number of recipients

1.090

Represent second fraction on number line as sum

63

0.652

Identify improper fraction from option 1

0.562

Identify number of items

1.206

Identify LCM - is product

64

0.734

Identify number as common factor

0.550

Rewrite fraction with common denominator

65

0.392

Identify number as common factor

0.367

Compare Options - operation

0.530

List consecutive multiples of a number

66

0.375

Identify number of items

0.363

Calculate sum – contextual

0.647

List factor of large number

67

0.561

Calculate Total

0.484

Label equivalent fraction in visual

0.518

Identify multiplier in equivalence statement

68

0.479

Enter quantity from diagram by calculating

0.345

Identify number of equal groups from fraction

次ページに続く

89

付録：タグ関係ネットワークにおける抽出タグと既存タグの対応関係

90

前ページからの続き
抽出タグ TF-IDF 値  既存タグ

1.222

Enter added quantity in diagram

69

0.749

Identify number of items

0.489

Identify number as common factor

0.397

Identify number of items

70

0.297

Identify number of items in each group from GCF

0.284

Identify LCM - is product

1.553

Identify multiplier in equivalence statement

71

1.456

Draw larger bar – multiplication

0.957

Identify number of equal divisions (horizontal bar)

72

0.774

Count number of shaded parts in vertical bar (discontiguous)

0.659

Calculate sum – non contextual

1.034

Identify negative integer from number line

73

0.687

Compare Options - operation

0.652

Identify improper fraction from option 1

74

2.749

Identify improper fraction from option 2

0.734

Identify number as common factor

75

0.611

Identify that a fraction cannot be simpliﬁed

0.485

Draw larger bar – multiplication

0.687

Rewrite fraction with common denominator

76

0.631

Identify number of items in each group from GCF

0.408

Identify larger quantity – addition

0.367

Identify improper fraction from option 2

77

0.367

Rewrite fraction with common denominator

0.337

Identify fraction associated with each piece of a circle

78

3.665

Compare fractions with unlike denominators

0.760

Represent ﬁrst fraction on number line

79

0.733

Identify that a fraction cannot be simpliﬁed

0.621

Identify common denominator

次ページに続く

90

付録：タグ関係ネットワークにおける抽出タグと既存タグの対応関係

91

前ページからの続き
抽出タグ TF-IDF 値  既存タグ

1.060

Identify Fraction using fraction shape

80

0.971

Draw larger bar – multiplication

0.489

Identify number as common factor

0.821

Enter total in diagram - calculated - addition

81

0.664

Count number of shaded parts in vertical bar (discontiguous)

0.623

Label equivalent fraction in visual

0.707

List consecutive multiples of a number

82

0.638

Identify number of total items

0.561

Identify fraction associated with each piece of a vertical bar

1.350

Identify number as common multiple

83

1.090

Identify fractional part of improper fraction symbolically

0.367

Identify number as common factor

84

3.347

Identify larger quantity – multiplication

0.631

Identify proper fraction from option 1

85

0.562

Identify number of items

0.430

Count number of shaded parts in square (discontiguous)

0.827

Draw smaller bar – addition/subtraction

86

0.636

Identify Fraction using fraction shape

0.636

List consecutive multiples of a number

0.842

Identify number of equal divisions (circle)

87

0.689

Enter quantity from diagram by reading

0.633

Enter larger inital in diagram – calculated

1.222

Identify GCF - one number multiple of other

88

0.916

Compare Options - operation

0.804

Identify LCM - is product

nan Count number of shaded parts in circle (contiguous)

nan Convert whole number to improper fraction

nan Represent second positive integer on number line as sum 89
nan Enter initial in diagram – given

次ページに続く

91

付録：タグ関係ネットワークにおける抽出タグと既存タグの対応関係

92

前ページからの続き
抽出タグ TF-IDF 値  既存タグ

nan Write mixed number as improper fraction

nan Enter group denominator

2.068

Identify negative integer from number line

90

1.674

Identify larger quantity – multiplication

1.590

Identify Fraction using fraction shape

91

1.456

List factor of large number

0.776

Identify common denominator

92

0.540

Identify number of recipients

0.458

Compare fractions with like denominators

0.638

Enter total in diagram - calculated - multiplication

93

0.484

Enter group numerator

0.345

Identify common denominator

2.179

Enter items denominator

94

0.734

Identify number as common factor

95

3.799

Represent ﬁrst fraction on number line

0.450

Identify number of items

96

0.392

Identify number as common factor

0.310

Count number of shaded parts in vertical bar (discontiguous)

97

3.665

Identify that a fraction cannot be simpliﬁed

1.178

Compare Options - operation

98

0.664

Draw larger bar – addition/subtraction

0.524

Identify GCF - one number multiple of other

0.522

Identify length of overlap

99

0.522

Identify proper fraction from option 2

0.359

Identify fraction associated with each piece of a square

1.915

Identify fractional part of mixed number symbolically

100

0.916

Compare Options - operation

0.870

Identify improper fraction from option 1

次ページに続く

92

付録：タグ関係ネットワークにおける抽出タグと既存タグの対応関係

93

前ページからの続き
抽出タグ TF-IDF 値  既存タグ

0.611

Enter added quantity in diagram

101

0.611

Compare fractions with unlike denominators

0.530

Identify Fraction using fraction shape

102

1.468

Identify number as common factor

1.123

Calculate diﬀerence – non contextual

103

0.500

Identify number of items

0.439

Identify fraction associated with each piece of a square

0.957

Copy initial in diagram

104

0.957

Compare Options - simpliﬁed

0.543

Identify number of equal divisions (square)

105

3.665

Compare fractions with like denominators

0.733

Enter added quantity in diagram

106

0.669

Identify larger quantity – multiplication

0.550

Rewrite fraction with common denominator

0.551

Identify number as common factor

107

0.545

Enter subtracted quantity in diagram

0.458

Compare fractions with unlike denominators

0.500

Identify number of items

108

0.484

Enter items numerator

0.484

Enter group denominator

0.929

Identify number of equal divisions on number line from desired denominator

109

0.636

Identify Fraction using fraction shape

0.550

Identify improper fraction from option 2

0.611

Compare Options - operation

110

0.516

Identify number of desired items

0.484

Identify number of items in each group

1.100

Identify improper fraction from option 2

111

1.010

Count number of shaded parts in square (contiguous)

次ページに続く

93

付録：タグ関係ネットワークにおける抽出タグと既存タグの対応関係

前ページからの続き
抽出タグ TF-IDF 値  既存タグ

0.827

Draw smaller bar – addition/subtraction

0.475

Identify that a fraction can be simpliﬁed

112

0.359

Enter total in diagram - calculated - subtraction

0.344

Compare Options - operation

0.442

Enter initial in diagram – given

113

0.335

Enter items numerator

0.335

Enter group numerator

0.965

Identify LCM - is product

114

0.760

Enter larger inital in diagram – calculated

0.733

Compare fractions with like denominators

94
以上

94

謝辞
本研究の遂行や本論文の執筆にあたり，非常に多くの方からご指導，ご支援をいただき ました．心より御礼申し上げます．
指導教官である松尾豊特任准教授には，研究構想の相談や論文の書き方，本論文の論理 構成について，貴重なご指導をいただきました．ここに，深く謝意を表します．
分析サーバや GPU 解析環境の用意等，物理的な研究環境の構築に多大なご協力を下さっ た研究室の教官である中山浩太郎先生に，深く感謝致します．
上野山克也助教授には，研究の方向性や論文の構成について，多大なご指導をいただき ました．深く感謝致します．
松尾研究室や GCI の皆様には，多大なご協力，ご支援いただきました．秘書の 中野佐 恵子さん，永本登代子さん，浪岡亮子さん，木全弥栄さんは，日頃から研究室の環境を整 えて下さり，研究生活を支えてくださいました．松尾研究室の博士・修士課程の先輩であ る 岩澤有祐さん，飯塚修平さん，野中尚輝さん，鈴木雅大さん，金子貴輝さん，那須野薫 さん，黒滝紘生さん，保住純さん，冨山翔司さんには，研究の相談に幾度も乗っていただ き，多大なご助力をいただきました．特に，研究テーマや，研究全体の設計について何度 も相談に応じていただいたことに加え，日常の議論を通じて多くの知識や示唆をいただい た那須野薫さんには，多大なるご助力に深く感謝致します．研究室の同期である大野峻典 氏，田村浩一郎氏は，卒業論文の構想や執筆に関して率直に意見を交わし，互いに切磋琢 磨し合いながら研究を進めさせていただきました．
ここに，松尾研究室の皆様へ謝意を表します．
東京大学工学部 システム創成学科知能社会コース
松尾研究室 学部四年 中川大海
95

平成 29 年 3 月 96

